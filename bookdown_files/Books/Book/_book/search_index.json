[["index.html", "R语言在心理学研究中的应用: 从原始数据到可重复的论文手稿 1 教学内容与课时 1.1 目录", " R语言在心理学研究中的应用: 从原始数据到可重复的论文手稿 胡传鹏 2023年1月5日 1 教学内容与课时 1.1 目录 第一讲：为什么要学习R（3学时） 1.1 R在心理科学及社会科学中的运用 1.2 R语言使用的示例展示 1.3 课程安排 1.4 如何学好这门课 第二讲：如何开始使用R:（3学时） 2.1 要解决的数据分析问题简介[介绍我们的数据和拟解决的问题，对比R和传统flow] 2.1 如何安装？ 2.2 如何方便使用？Rstudio的安装与界面介绍 第三章：如何导入数据（3学时） 3.1 路径与工作目录 3.2 读取数据 3.3 了解R里的数据 （R语言中的对象） 第四章：如何清理数据一 R语言编程基础（3学时） 4.1 R对象的操控 4.2 逻辑运算 4.3 函数 第五章：如何清理数据二 数据的预处理（3学时） 5.1 数据预处理准备 5.2 数据预处理的基本操作 5.3 数据预处理的进阶操作 第六章：如何探索数据: 描述性统计与数据可视化基础（3学时） 6.1 描述性统计 6.2 ggplot2的基本使用 6.3 ggplot2的元素控制 第七章：如何进行基本的数据分析: t-test和anova（3学时） 7.1 语法实现 7.2 分析的流程 第八章：如何进行基本的数据分析: 相关与回归（3学时） 8.1 语法实现 8.2 分析的流程 第九章：如何进行基本的数据分析: 中介分析（3学时） 9.1 语法实现 9.2 分析的流程 第十章：结果稳健吗？使用Multiverse比较方法选择对结果的影响（3学时） 10.1. 多种分析方法的实现 10.2 代码整合与规范化 第十一章: 如何得到可发表的图像: 数据可视化进阶（3学时） 11.1 ggplot2的图层与面板控制 11.2 ggplot2与其他工具的结合 第十二章：从分析到手稿（3学时） 12.1 Rmarkdown 12.2 Latex语法基本介绍 12.3 papaja工具包的介绍 第十三章：多人协作版本控制:Git？（3学时） 13.1 版本控制与git 13.2 多人协作与git 第十四章：如何帮助我们计划下一个研究？（3学时） 14.1 计算效应量：Meta-analysis 14.2 计划样本量：Power analysis （模拟） 14.3 计划分析方法：假数据与分析代码（模拟） 14.4 并行处理 第十五章：如何让导师/合作者完全重复我的分析？（3 学时） 15.1 软件版本记录 15.2 容器技术与docker的使用 "],["lesson-1.html", "2 第一讲：为什么要学习R 2.1 R在心理科学及社会科学中的运用 2.2 R语言使用的示例展示 2.3 课程安排 2.4 如何学好这门课", " 2 第一讲：为什么要学习R 序 本次课的主要目的在于帮助大家了解本课程的基本情况，为本课程做好心理准备。所以接下来，我们主要介绍为什么要开设这门课程、课程的内容是什么、需要做什么样的准备、以及能收获什么。 2.1 R在心理科学及社会科学中的运用 2.1.1 数据科学 这门课的开设有其时代的大背景。作为在心理学院的课程，我们将这门课称为《R语言在心理学研究当中的应用》。但实际上，R语言是当前数据科学（data science）中主流的计算机语言之一。 正是数据科学在各种学科中的渗透和普及，让我们开设这门课程成显得非常重要。那么什么是data science呢？ 数据科学是什么？ 在科学研究中有人认为，科学的革命是经过了几次范式转换的（参考链接）。最早期的是”实验”科学，研究者通过设计和完成实验，一个一个地去检验假设。随后是理论科学，在实验基础上进行归纳。随着计算机越来越发达，我们进入了”计算”时代，通过用各种计算模型模拟的方法，帮助我们去理解世界。但是现在，随着数据越来越多，通过数据驱动的方式就能发现很多新的东西。最近这些年，很多在科技领域尤其是在计算机领域取得的重大突破和进展都是依赖于大量数据的，也就是通过对数据进行“提炼”从而得到新的发现。比如说最近(2023年初)非常火的ChatGPT。作为现在全球最火的科技界产品之一，它背后的模型叫做LLM，Large Language Model。这里说的Large language就是一个大语言模型，它依靠的就是大量语言材料的训练。 数据科学的内容 大概10多年前，数据科学就已经出现。大家也许对“数据科学”这个术语已经不再陌生。数据科学里面既涉及到计算机编程，也包括数理统计。当讨论具体应用领域的数据科学，比如心理学的科研领域，数据科学也需要domain-specific 的知识，也就是这个领域的特殊性知识。 [此处可以插入关于数据科学的Venn图] 这意味着什么？意味着如果你仅仅懂计算机，那你不一定能懂data science的；如果你仅仅是懂数学和统计，那也不意味这你能解决一个data science的问题。必须要将计算、统计和领域特殊的知识进行结合。在心理学研究中，这对研究生提出一个新的要求。 2.1.2 数据科学的诞生——数字化时代 为什么会有data science？大家应该能直观地感受到：随着个人电脑的普及，互联网越来越发达，整个社会所产生的数据呈现爆炸式的增长。下图是一个可视化的例子。我们可以看到，在计算机出现之前人类产生的数据是非常少的，而计算机出现之后产生的数据越来越多。 我们也有了越来越多的个人电子设置以及其他的先进设备，它们所观察到的、产生的数据也是非常大的。去年的这个图片相信很多人在朋友圈都被刷屏过。这是人类所能观察到的一个划时代的新的图像，尽管我们作为外行可能不知道它具体的内涵是什么，但是都知道它很酷。 此外，我们国家是互联网普及最高的国家之一，我们现在有百分之七十四（可能现在又更多了）的人都已经开始接入到互联网。这么多的人接入到互联网，所产生的数据可想而知，一定是海量的。所以现在我们很多电商，像淘宝这样的各类购物平台，它们在中国做的是非常好的，这也得益于海量的数据。包括最近像拼多多，听说在美国也是势如破竹，态势很猛。还有像TikTok，就是字节跳动，前一段时间在美国甚至被封杀了，为什么它被封杀了，因为大家都很喜欢他。就比如中国的这个字节跳动，它有一个特点就是它很好地利用了中国大量的网民产生的海量数据，通过网民不断地使用它们的产品，不断地进行迭代。所以当它能出海的时候，去给海外的用户提供服务的时候，它的迭代已经非常成熟了。当然迭代的过程中需要大量数据的产生和调试，产品才能越来越成熟。 数字化对心理学研究的影响 其实很早就有人关注数字化我们心理学的影响了，国内的研究者也经常会提到心理学和大数据。我们这里做一个不完全的概括，主要是这三个方面的显著变化。 Big n (sample size) 首先就是样本量很大。现在的数字化平台生产的数据非常大。我们传统的行为学实验可能就是几十、几百的数据，上千已经是很不错的了，上万就比较费劲了。但是如果说我们能从互联网上抓取数据的话，那动辄就是上万甚至是百万级别的。 Big v (variables) 不仅仅是样本的体量更大，也意味着收集到的变量更多了。比如我们使用手机，那其实我们产生了非常多的数据。你一天使用多久、点击多少次、点击了什么、在哪个地方、用的是什么APP，甚至包括你所处的地址，你在地球上的经纬度、当地的气温、湿度这些数据信息。如果大家还记得的话，有一有段时间有一个非常火的一个图片。显示的是美国陆军还是海军运动的一个轨迹。因为他们使用了一个运动的APP。大家就可以看到他们在在哪跑步在哪个健身房健身，然后去哪活动等等非常的详细的地理的资料。当然这是属于一个看起来很好玩的东西。那么能不能够用于心理学的研究呢？几年前，北大的王连老师和我们心理学院的韦文琦老师在human behavior上面发了一篇论文就是讲不同的地区跟人格之间的关系。所以实际上就是我们能够产生很多的数据，这些数据能进行大量的挖掘，这些挖掘可能是以前你没法想象的。 Big t (time) 还有就是时间的跨度比较长。现在很多的APP一旦用户开始使用之后就会长期使用，如果能用于收集心理学的数据，就可以在很长的一段时间里记录很多的数据。这对于了解人类的心理和行为的规律来说其实是非常好的。对于发展心理学家来说，有这样的一个例子：有一个做语言发展的一个研究者他从自己的孩子出生开始就一直用视频记录孩子的成长。儿童产生语言目前对于人类来说还是一个不能够完全理解的过程，从语言学上看这是一个很大的跳跃。这位研究者完全记录了孩子生长的过程，积累了大量的数据，并通过对视频数据的全方位分析，得到了以前心理语言学比较少能够得到的东西。 数字化时代的心理学研究 这里有几个例子。比如这个是利用手机里的数据预测人格，我们这里看到的纵轴就是人格的类型，不同的颜色表示使用不同的方法进行预测，横轴是相关系数。可以发现说手机里的一些数据和人格的倾向是密切相关的。 另外，像我们心理学的顶刊Psychological Science，上面也不定期的有研究是探索我们在数字的视觉中留下的痕迹跟我们的行为之间的关系。比如这篇文章发现我们个体在使用手机的行为上是非常的一致的。研究者通过大量的数据是得到了一个比较强的一个结论，尽管这个结论比较简单，但是基于数据量比较大，说服力是比较强的。 我们也可以利用互联网的平台来收集行为的数据。传统上我们是把被试带到实验室做实验，但现在由于互联网的发达，我们可以进行线上实验。在疫情期间，不少同学可能尝试过JsPsych或者巴普洛夫这样的平台，把实验编写好放在网上，并通过链接给被试发放被试费。这样我们在短时间内可以收到比传统实验室更大量的数据。同时，因为我们使用的其实还是传统的实验任务，只要事先验证过在线的实验和实验室实验的可比性，就可以利用互联网在线去收集更大量的数据来研究我们感兴趣的问题。 比如这里列举的实验，去年的R课上我们以此作为了示例。它就是通过在线的平台收集的数据，收集到了很多变量。它的研究一个目的是为了探测self regulation，自我控制或者硕士自我调节，不同的测量方法是不是一致的，哪一个能更好地预测生活中的一些行为，比方说这里就是看哪一些与饮食控制有更强的相关。 数字化时代心理学研究方式的变化 数字化时代不仅给心理学提供了新的数据，实际上也变革了我们做研究的方式。比如说合作。在疫情以前，研究者建立国际合作主要是通过导师的联系、邀请讲座或者是开会时建立联系。但在疫情期间，因为大家都困在家里同时又都使用很发达的互联网，所以就有研究者直接在社交媒体上发起合作的倡议。比如有的人说我有个想法需要在不同国家收数据，想要有人和我一起收集数据，最后可以一起发表文章。在疫情期间很多研究就是这么展开的，并且因为这样的研究往往样本量比较大，也能发到很好的期刊上去。同时，除了个人的发起，研究者们也成立了更多的学术组织，比方说Psychological Science accelerator心理科学加速器，这个组织就是专门组织在全球范围通过互联网进行合作研究。 此外，以前研究者需要去线下开会或者是参加工作坊，现在即便是疫情过去了，大家还是越来越习惯和更多地采用在线的方式进行学术讲座或是工作坊。所以呢互联网实际上是改变了我们心理学的方方面面。 2.1.3 为什么要学习R语言？{1-why-learn-R} 我个人总结了一些我们学习R语言或者说学习R而不是matlab或者python的理由。 首先是一个大的背景。R语言是一个开源的一个软件，他跟python、junior一样，基本绝大部分的基于R语言的工具都是开源的免费的，也说你基本上都能够（只要你的互联网是畅通的话）免费得到所有的内容。 第二，它是一个高级的语言，不需要和计算机的硬件直接进行交流，和我们日常的语言差不多。 第三，它有一个强大的community。因为现在的所有的开源的语言，他依赖于有多少人在使用它、有多少人在不断的进行开发、尤其是谁在开发这些新的东西。对于我们心理学或者是社会科学而言，绝大部分是使用R做数据分析。简单来讲，我们一开始作为新手肯定不会去开发什么工具的，就必须要把别人开发的工具拿过来。那谁为我们开发呢？肯定是这个community里的人，这就需要我们有一个比较成熟和强大的社区。而R语言本身就是由统计学家所开发的，所以它就是为了做数据分析而生的一门语言。同时，在这么多年的发展当中有大量的研究者，尤其是社会科学的研究者不断加入这个community，从初学者变成使用者最后变成开发者。 从做研究的角度来说，R可以在这三个方面提供强大的支持。 科学性 它有助于增强计算的可重复性，帮助我们找到更加适合的新的统计的方法。 美观 它可以帮我们得到非常好的图片，有非常精细的图片的细节控制。 实用性 它可以适用于我们做研究的各个阶段，也能够适应这个数字化时代的需求，比如大数据的需求。 2.1.3.1 心理学的可重复性危机 在我们心理学领域从2011年开始出现了一个比较大的问题，就是可重复性的问题。大量发表的研究的结果无法被其他的研究者独立重复。那这个问题到底有多严重呢？最有代表性的应该就是这篇文章。在2015年的时候，一篇Science的文章专门报道了整个心理学领域的可重复性的问题。Science是跨领域的多学科的一个综合的期刊，能够发表到Science这个期刊的文章都是能够引起广泛的兴趣的，也是对整个科学界来说都很重要的。在这个文章中当中100个团队重复了2008年发表在心理学顶刊上的100个研究。他们的分析的发现大概只有36%的结果是能够被重复出来的。2015年这个结果是引起了非常大的震撼也被nature评价为2015年的年度的十大重要的论文之一。因为这个问题出现，研究者就做了很多的反思。当然我也是被这个问题所深深的震撼，现在还是在一直在寻求能够去做到更加严谨、可重复的透明的这种研究。这个是我在2016年的时候跟我们课题组的同学一起写的一篇对可重复性问题的一个介绍和思考，大家有兴趣的话可以看一下。 我这里没有去把这些心理学的计算上可重复的研究拿过来，有人对心理科学在science这个期刊上面有公开数据的文章进行了计算的可重复性的检验，也就是说按照研究者描述的方法去做一遍分析，看能不能得到跟研究者一模一样的结果。大家猜一下这个比例大概有多高。做一个区间吧30%以下、30%到50%、50%到80%、还是80%到百分之百。约为30%以下的举手，30%-50%呢，50%到80%呢。大家都很乐观啊，80%我就不问了吧。如果说我们考虑完全能够重复的话，他们在14篇文章里面只有1篇能够重复，是1篇还是2篇能够完全重复。然后有的是在作者的协助之下都得不到原来的结果，所以这个问题并没有那么简单。 2.1.3.2 利用R语言增强计算的可重复性 既然他是这么大的一个问题，那么为什么说r语言可以帮助我们解决计算的可重复性呢？首先是说可重复性它是有多个层面的。大家可以可以想一下，如果说你的这个结果是可以重复的，那么最简单的一个可重复是什么？就是计算的可输入性对吧。computational reproducibility，这个computation reproducibility说的是什么？假如你有一个数据，然后你做了一套分析，你把它报告出来了，我拿到你这个数据，我按照你描述的方法，我能不能得到跟你一模一样的结果。假如说你的计算的过程当中，没有一些随机的生成的过程，全部都是说我们用的这种可以求到解析解的这种这种算法的话，那就意味着我不仅仅要跟你的结论是一致的，而且是在数值上应该是一模一样的。你原来得到比方说t等于2.1，那我应该也得到就是t等于2.1，或者你得到的是f等于 10，我应该也得到f值等于10，不然就说明什么计算上他是他是不可重复的。 2.1.3.2.1 记录数据分析的全过程 你们现在可能绝大部分是研一的同学，那么你们自己有做过数据分析吗？有的举手做过数据分析吗？就是你们做毕业论文应该做过数据分析吧，都不举手，没有做过毕业论文？做个数据分析大家都是用的什么软件，SPSS？那你们都要通过手点击吧，你们现在还记得怎么点击的吗，一步一步的怎么点击过来的。 所以我们实际上按照传统的做数据分析的方法，我们都是用手动的去点击对吧，尤其是前面那一部分。我们不是说你把数据录入到SPSS是以后的那部分，（说的是）比方说你用你在问卷上面，用问卷星收一批问卷，那里面可能有一些不太认真的吧，你要把它给删除删除掉对吧？有可能你会删除一些你认为是比较极端的也确实可能是极端的数据对吧？有可能你就是100个人里面或者是300个人里面你把某一两个（极端数据）你当时看到你就删除掉了，然后你最后认为你得到了一个干净的数据，你把它存起来以final或者是以最终数据作为后缀对吧，然后你就会基于那个数据把它打到SPS里面对吧。但是如果说你要重复的话从前面到你那个最终数据你能够（重复出来吗？）可能一个月之后你就不一定记得为什么你删除某个数据了。那么这是很普遍的一个（原因）导致我们最后结果无法得到（重复结果）。如果我们用R语言编程语言来记录数据分析流程的话，就可以把我们整个数据分析的过程全部记录下来，也就说任何一个步骤出错了我们都可以找到，因为我们代码全部在那里。 我们这门课最后就是会从实验软件导出的数据出发，这是最原始的数据，以那个数据开始怎么预处理，一直到我们后面得到可分析的数据，一直到后面做统计分析。我们会展示如何把每一个步骤都用代码记录下来，这样一个好处就是即便过了一两年之后，即便我们已完全忘记了当时是怎么处理的，但是代码还是可以告诉我们当时怎么做的，这一点就可以帮助我们去保证计算的可重复性。 2.1.3.2.2 跨机器的一致结果 另外一点，可以帮助我们达到跨系统或者是跨机器的结果。这个其实是在我们心理学的数据当中比如行为学的数据当中是不是很大的问题。为什么呢？因为我们行为数据的处理涉及到的步骤很少，即便里面包括一些随机化的过程，他的错误不会累积和放大。但是如果你们要去处理一些分析流程更长的一些数据，比方说像fMRI的数据，那么你在不同的机器之间的随机性或者浮点数据导致的这个差异，他就会随着你研究的步骤慢慢积累起来，也就是说即便你的这个系统刚开始的时候输了原始数据。经过了不同的系统不同的机器有不同的随机的非常微小的差异，经过一段时间之后也会累积成为很大的一个差异。我们后面会讲如何控制这种随机性导致的这个结果，如果我们使用比较好的使用包括像pandas？或其他的一些软件，我们实际上是能够达到某种程度上跨机器的一致性的。当然到了一个精度非常高的程度的话，其实就不是我们心理学家能够解决的问题，因为他涉及到一些计算机内部如何去控制浮点的精确度等一些技术细节的问题。但是我们可以怎么样呢？当我们学习了这些编程语言之后，我们能够去把计算机科学家在这方面做的改进纳入到我们的分析当中从而去改进我们自己的分析的结果。 那么在这个记录分析的全过程中我自己有一个例子。就是我们在2020年有一篇文章当中公开了数据。但是呢最后其实我们的数据跟统计的结果稍微有一点不一致，有一个读者他读了我们文章之后给我发邮件，我们后来是追溯到内部。之所以出问题就是我们用Excel来操作的，然后我们用Excel操作的时候不小心删除了几行。所以如果我们使用R做全部的数据分析的话，应该就不会出现这个问题，这是说R可以做全过程的一个记录。 那么另外一个例子就是我自己在2020年另外一篇文章当中，因为他的数据和代码全部是公开的，所以有一个叫做reproducible的团队，他们去对已经发表的这个文章的结果的可重复性进行评估。他们就对我的这个数据和代码尝试进行了一次重复。那么他们就是说行为的这个结果绝大部分是能够重复出来的，但是有一部分是没有重复出来，原因是因为他安不安装不了那个软件。这不是一个很小的问题啊，我后来花了两年的时间去专门把那个软件打了一个docker的包，我们后面会学到的这个docker就是为了去让我们能够保证跨机器的一致性。这个看起来是个很简单的问题，你发了一个文章，然后你做了某一个分析，结果别人连你的软件都装不上。那么我们如何去提高自己的研究结果的可重复性，那当然就是我们把软件让它变得更好安装。那个重复的它是用python写的，不是用r语言写的。他重复这个R语言部分呢，他的comments都是非常好的，就是说即便他没有在他自己的工作中没有使用R，但他也能够很好的读到我的（注释），他也能够知道我在做什么，非常详细。大家以后能够比较好的比较规范的写自己的r代码的话，那么同行的反应也是类似的，他会发现你的这个数据，第一个结构非常清晰，第二个你的代码非常清晰易懂，他很很快就知道你在干什么，他也能够很快的重复你的一个结果。至少这样的话，你就保证了自己的结果是非常的稳定的也非常靠谱的。 2.1.3.3 R有更合适的新方法 2.1.3.3.1 强大的使用群体和不断更新的方法库 然后另外一个就是说我们在用r因为他有一个比较强大的社区使用群体，并且不断有人在开发新的方法，这就意味着我们可以不断的使用更加新的方法，而不是拘泥于我们在课本上学到的比方说常规的这些统计test、anova、线性回归。我们可以使用一些更加适合我们研究问题的方法。 2.1.3.3.2 可使用最新的方法 那么我在这里列举的一个方法，我们后面也可以在课堂中进行实现的，那就是IJzerman2018年的Collabra这篇文章。那么这篇文章我也是合作者之一，当时也通过互联网我们来合作收集的数据。在这个文章当中，他就使用了机器学习的方法，叫做（条件）随机森林，叫做conditional random forest。他实际上是在机器学习里面非常常见的一个方法 他的特点就是说即便你只有比较小的数据，你也能够得到比较稳健的一个结果。当然这个小的数据是相对于机器学习里面的小的数据，因为机器学习里面可能动则就是上十万百万的数据。相比而言，我们的数据其实每一个都是很小的，就几百人上千人。所以当拿到这1,000多人的数据之后，他想去探索这么多变量之间到底哪些变量之间有一个比较稳定的关系，他就采用了随机森林的方法，最后也发现他感兴趣的那个变量，就是身体的温度和这个社交网络的复杂程度是有关系的。 2.1.3.3.3 可使用更合适的方法 另外一个就是比方说我们实验室实验当中非常常用的反应时间，它基本上都是偏态的一个分布，对于这种偏态分布的数据我们到底应该采用什么样的一个模型，到底是用传统的线性模型还是应该用广义的线性模型。如果说我们是使用r，那我们可以很灵活的使用r里面比较新的一些回归模型的包。在这包里面我们可以使用最适合这个模型的，比方说GLM。我们甚至可以通过模型比较的方式找到哪一个模型是最适合的。也就是说正是因为在r里面有一个很强大的community，然后这里面有众多可以选择的r的工具包。这样我们就能够不仅仅是使用新的方法，它也可以帮助我们不断的去选出更加适合的方法。 2.1.3.3.4 R更合适可视化 R语言绘图可调节每个细节 然后从美观的角度来讲，R画图可以精确的调节每一个细节。这个我们后面再讲可视画的进阶，那一章的时候我们会把那个ggplot这个最常见的画图软件里面的每一个细节都掰开讲，这里我们只是稍微展示一下。 然后这个最简单的图，box plot就很一般了。我们可以把原始数据和group level数据结合到一起，然后再把每个被试的数据，把它的分布画出来。 最近几年非常流行的雨云图。当然我们还可以把多个（图）进行叠加，像这种被试类的实验设计我们可以把每个点都连到一起，可以看到在不同之间的一个变化，它是不是完全具有一致性的。然后我们把把这个box plot也加上去，这样的话我们能够看到极端点。我们同时还把这个分布加上去，当然这个分布目前的α值比较高，我们还可以把它调的低一点，就是说让他透明度再低一点，让我们看到这个分布之间的一个叠加。这样我们就可以在一个图上看到非常丰富的信息。 当然还有一个叫做ggrudges的一个图，这个上面我们不仅仅看到可视化的效果，还可以直接把值标到上面。这样一个图给我们的信息量就非常大，当然在画图的时候我们不是单纯的追求这个信息量很大，我们要美观。要有足够的信息量同时也能够让大家不会一看到之后就不想看了，而是说看到之后能够立刻get到你想要传达一个什么样的想法，这个是很重要的。所以可视化这一点上面说实话我们即便在这个课上有两次课，但是我们只能教大家一些方法，大家最后画图的实际效果要依赖自己的taste，就是自己的一个口味和不断提升的感觉。 我们也可以把很多distribution把它进行叠加，叫做GG Region，我们也可以画地图。我们可以把一些相关的数据在地图上进行映射，随着我们越来越多的能够得到不同地区的数据，把这些数据映射到地图上的时候，我们就会发现很有价值的信息。这个图是我最近画的一个图，就是我们在分析大团队科学中的样本被试到底是不是真的具有代表性。因为很多做这种就是跨国的研究的研究者总是会claim我们的这个研究从几十个国家来的，那么这个数据是能够推广到全人类的。是不是真的如此呢？我们看一下被试在我们这个图当中（的位置），我们就这边是中国的人口的一个（分布）那边就是他的那个被试的群体在不同省份的一个分布图，我们可以看到其实他选取的样本主要就集中在这两个地方，一个是广西一个是上海，其他地方的话其实数据量非常少。 当然我们还可以从其他的维度对他的样本代表性分析，这里主要是展示我们可以把数据映射到地图上面，这样一眼就看到他的数据到底行不行。现在把数据映射到地图上是越来越多的使用在心理学的理念当中。 2.1.3.4 R的实用性 2.1.3.4.1 R的实用性之一：适用于数据分析的各个阶段 几乎科研每个阶段中涉及到的数据处理，均有对应的R包。 我们刚刚说的是他能够帮助我们增强计算可重复性、能够帮助我们使用更好的方法、能够帮助我们得到比较漂亮的适合我们目的的可视化。那么还有一个就是r是非常实用的。目前基本上我们数据分析的各个阶段都可以使用，比如说我们如果使用r就是整个数据流程一条龙就下来了。我们到课程的后期，会介绍如何把我们的数据分析和我们的代码以及我们的论文的描述文字直接把它整合到一个文档里面，然后生成一个PDF。这个PDF就是可以拿去投的PDF。 我们从原始的数据，然后再读取数据，然后得到一个比较整洁的数据，然后这个整洁数据我们就可以进行可视化，然后也可以进行统计分析，最后我们就可以到word文档，甚至可以得到PPT，就可以直接给大家展示。当然要掌握这里面的每一个流程的话其实是要花很多时间的，但是呢我们可以找到一个对心理学研究者来说最快的（方式）。比方说我们这个课上就会把整个过程中所有心理学的数据整个演示一遍。大家就可以照着这个流程去走，就不需要再去重新探索，这是我们这门课的意义。 那么其实每一个涉及到数据处理的科研阶段，我们前面讲的是数据分析，实际上对应就是我们这个阶段得到的数据之后我们分析数据检验我们的假设。但是在我们的研究中其实我们在实验设计的时候，我们也需要数据分析。我们在设计实验的时候我们需要知道我们大概需要什么样的一个实验设计。比方说我们是做问卷还是说我们要做一个实验，那么做实验被试间设计还是被试内设计，然后被试间设计或者被试内设计之后我们需要多少被试。这里就会有一个大家通常会碰到的一个问题就是power，叫statistic power。我们如何去确认我们得到的power它是符合我们的目的的，实际上也是可以通过R语言的一些相关的工具包来解决。还有比方说我们在quality control里面其实也涉及到我们如何去处理这个缺失值。比方说你收了一批数据，80%的数据都都是没有用的，那他到底算不算一个好的研究？或者说我们有少量的缺失值的时候我们应该如何处理缺失值。实际上他既是属于质量控制也是属于数据分析中的一部分，然后另外一个实用性是可以适用我们现在这个数字化时代的需求。因为我们现在越来越多的大的数据，所以我们要使用一些更加fashion的一些方法，机器学习、深度学习什么的。那么r语言现在已经有很多这种框架了，如果我们能够掌握r的知识以后我们后面去拓展到这些部分相对来说是容易的。因为里面已经有一些比较成熟的框架那么就像我们能够调用Tidyverse，我们同样也能够去调用这些机器学习的包，只不过我们要真正的合理的使用还是需要去了解了解背后的一些知识，不能盲目的使用R语言。 2.1.3.4.2 R的实用性之二：适应数字化时代的需求 广泛使用 他（R）现在也是一个非常广泛使用的语言，大家可以看到Python可能是非常火的一个语言，还有像其他的Javascript还有各种各样的。像在这个表里面能够做数据分析的大概就是python然后r、matlab，其他的可能都是用于其他功能的一些软件了，也就是说我们如果使用r的话它的最后的这个功能也相对来说是比较朴实的。你可以用它做很多事情包括你可以用它建个网站。如果大家有看过我们课题组的网站的话就是huchuanpeng.com就是用r做的，是我们助教之一胡孟真做的。 强大的社区、众多的教程 我们有一个非常强大的社区。意味着有很多人教你做各种各样的事情，也就是说你如果你想做什么东西，99.9%的情况下你不需要自己去真的去从原理到到实现全部实现全部去做一遍。而是去搜索前人是怎么解决的。比如说你要就要做meta analysis你就搜索meta analysis视频，然后你能得到很多的教程，这时候你就去找一个好的教程就可以了。或者比方说我们要做混合线性模型，你就搜索一下肯定又会得到很多教程。 课程意义 这门课的意义是什么呢？我们这门课的最主要意义在于让初学者从完全不会到能够不害怕使用R。这是我们这门课的最大的意义。在心理学研究当中R语言也是慢慢地变得越来越流行，像我们在有一些比较新的期刊，他会发表很多教程性的文章，就是专门教大家如何去使用各种各样的新的方法。其中有多篇关于R语言的使用的。 2.2 R语言使用的示例展示 我们已经讲完了第一部分关于为什么要学习的内容，希望大家在听完后仍然能够有学习的动力。接下来，我们将简单展示一些使用R语言的代表性情况。你现在看到的是其中最具代表性的情况之一，即遇到错误的情况。例如，在使用R语言时，实际上碰到错误的概率几乎是百分之百的，而即使你非常熟练，仍然可能出现很低级的错误，比如漏掉一个字符或者反引号。我们将在之后讲到数据类型时介绍不同数据类型需要对应的一些符号。 2.2.1 数据清洗 在数据清洗方面，我们一般会使用dplyr，它是Data science里面非常常用的一个功能，需要进行各种数据转换、分组等等操作。但是数据清洗通常是数据分析中耗时最长的一个过程，即使是简单的数据也需要花费相对较长的时间进行清洗。虽然在使用SPSS的过程中，我们已经形成了一个非常快速的数据分析思维，但是在使用计算机语言进行数据分析时，这个过程完全不同，需要花费大量时间进行数据清洗。即使是行为学数据，甚至是简单的反应时数据，也需要进行相对较长时间的数据清洗。 这里面可能有几个图没有截过来，在数据科学中，无论你从事哪个领域，完成一个数据分析项目的时间通常会包括从数据清洗到最终分析以及报告撰写。其中，数据清洗通常会占用至少60%的时间，这个过程可能需要反复查看和修改。传统的做研究的方式并不习惯分享数据，因为整理和清洗数据需要很长时间。即使你的文章已经发表，清洗数据所花费的时间也可能会被认为是浪费的，尽管有时会带来间接的回报，如其他人可能会重新使用你的数据或发现你的分析的可重复性很高。 2.2.2 ggplot2画图 另一个耗时的任务是画图，尽管这些图看起来很漂亮，但需要不断地调整和修改。有时，研究者会花很长时间去完美地绘制图表，而这些时间可以用来完成其他任务。经常会有研究在这个社交媒体上，他自己比方说做统计分析发了一分钟，然后画图画了两天的时间，就是不断的去调，并不是他不会画图，而是他总是觉得画出来这个图不满意，然后就不断的调整，不断调整最后发现，时间就没了，并且呢你也会发现呢，就是当你掌握了不同的这个，画图的这个方法以后呢你会不断的去想，我能不能找到一个更合适的方法，去对他进行更好的一个可视化，我没有来得及把我自己之前的一个画的图的这个历程贴上去，我刚开始就是最简单的这个，跟我们在，Excel里面画的那个图是一样的，就是一个直方图上面加一个error bar，这是我一开始用来画的用APA的格式，后来就变成那个带散点的，再后来变成了那个raincloud，然后就是raincloud加box，然后加这个distribution，然后最后又回到了这个散点的，加上这个主题，就是groups这些。所以其实这个画图的时间需要大家当你熟练了以后也需要适可而止，差不多能够传达你的这个信息就可以停止了，要不然的话，这个画图的提高是没有止境的，有的也可能可以看有的时候也看到，就比方说一些好的期刊，比如像经济学人他们涉及到数据的时候，像比方说，我们说Nature、Science或者PS对吧，nature Communications，当他们涉及到数据的图的时候，那都是非常漂亮的，包括他们配色包括的比例各方面，他实际上都是经过了，就是专业的人士进行调试的。那么有的时候其实我们要想要达到这个类似的效果的话，也需要花很长的时间去做这些细节的工作。但假如我们只是按照心理学传统的做法，Excel的那种一个bar图加一个error bar的那种非常快基本上几行代码就可以搞定。 2.2.3 心理学数据分析与结果汇报 针对于各种各样学科、心理学的这个数据的分析的包，也有心理学的研究者开发的包，例如蔡华杰老师的PLUS包，该包专门针对心理学数据分析，并包含很多有用的功能，非常适合心理学学生使用。例如，对于做T检验的研究，可以使用该包的功能，我们可以用他的这个T-test对吧,它可以将结果输出成为一个简单的三线表格，并且可以直接在命令中加入指令将结果输出成为一个Word。这个word文档里面就有这么一个表。你就可以直接复制打开到你的文档里面，这个是非简单的。他也告诉你，你的零假设是什么，我们这个是单样本的t检验，那么他告诉你这个假设就是双侧的，然后这个叫做均值，他不等于700，那么类似的可能我们还有其他的，比方说我们用这种配对样本t检验对吧，他同样的也可以得到这种非常适合我们输出的这个结果，而且他最近也把这个face back进去了，所以我们大家可以看到这里报告的信息肯定是比spss更加全面的。 2.2.4 Regression 那么另外就是关于这个regression这里面我们其实没有使用BruceR，如果我们使用BruceR的话，他有一些专门的回归模型，然后我们这里有个简单的回归模型，把回归模型直接做一个这样的输出，Bruce也是可以的。还有这种，这里应该也是一个简单的回归模型，还有就是我们也可以使用这个SEM，这个地方的话，我们可能要使用一个新的包，就不是bruceR，当然bruceR他也把那个，如果没有记错的话，把SPSS那个process整合进去了，这里我还没有尝试过，可能我们后面可以一起来探索。 2.3 课程安排 我们这学期的课程的有三个原则：第一个就是即学即用，我希望我们在课堂上教的这些代码，大家都能够在自己的数据分析中使用而不是我们在这里学了一遍之后，然后自己需要重新去学，或者这里面学的代码，都是大家用不上的，这个我们尽量避免。因为大家时间都很有限，如果能够帮助大家省一点时间的话我觉得或者少走一点弯路的话，减轻大家的心理负担。 第二个就是在做的过程中学习，比如说我们当我们前面的两节课，第一节课我们是介绍对吧，第二节课我们就是教大家怎么去安装，然后去了解这个里面的各种各样的功能，以及数据导入，以及各种各样的情况，基本上第二节课以后，我们就开始就直接就给大家讲这个代码。我们需要在做的过程中学习。 然后第三个就是逆向学习，逆向学习就是你先做，你先能够在哪里面实现这个东西，然后我在这个我演示的过程中，我用一个命令，比方说就是t test或者f test对吧，我能够得到这样的结果，大家首先说的就是你在你自己电脑上面，或者说你在这个云计算平台上面你能够实现这个功能，你能把这个代码抄下来，然后呢得到跟我一模一样的结果。然后你后面再去理解他，他到底为什么这样或者说如何去使用他，你先会做然后再去理解，这个是对于学习代码来说，我觉得是很好的一个一个做法。因为如果你看到你一两本书之后，你其实没有写过几行代码这个实际上是非常的浪费时间的一个事情，尤其现在大家的这个时间也比较紧张对吧，然后呢我们的一个目标就是想要去压平这个学习曲线，因为如果大家知道学习，就是说以前大家认为这个R语言的学习曲线是非常陡峭的，就刚开始特别难，你要进步的话很慢，就是很难，也花很多时间。我们希望他尽量的快一点，然后可以慢慢的去后面的不断的学习。 那么参考教程的话，英文版的有这个（Naverro, Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1),https://learningstatisticswithr-bookdown.netlify.app），然后中文版的话，有一个叫做王敏杰老师的《数据科学中的r语言》，他实际上是一个公开的一个教程，那么大家可以把它当做参考书，因为这个书里面讲到了非常多的一些知识点。但是我们不会按照他的这个知识点进行讲解。然后另外一个呢是叫做Tidyverse，张继星老师，给大家展示一下，另外一本书，这个我最近也买了。那他实际上跟我们的课堂的这个内容契合度是非常高的，因为Tidyverse就是我们最常使用的一个工具包，那么课程的安排的话我们就是，我们课程的内容的话其实就基本上跟我们的研究密切相关，因为大家都是研究生对吧，所以我们是希望：首先是跟数据分析这部分密切相关，然后等我把这部分解决之后，我们看到后面进阶的话，在设计实验的时候怎么把这个部分做好，那么在数据分析的过程中的话，我们就会有一个完整的流程：我们从原始数据到清理，然后到数据的探索，统一分析，然后分析完了之后统一推断，然后把它结果去验证，然后撰写一个报告。 我自己本人也非常喜欢可重复性和开放科学，因为我觉得他是科研中很重要的一个方面，我们后面也整合了一些有关我们如何跟他人进行协作的内容，帮助他人一起共同的合作来完成任务。还有如何保证我们的可重复性，如何采用一些更加先进的计算机技术，来帮助我们更好的保证我们的这个计算的可重复性。然后以及如何直接能够从代码到数据，生成一个PDF文件或者word文档，能够生成一个直接可以提交的一个版本。 2.3.1 课程大纲 我们不会按照传统的这种介绍R的方法：先介绍R里面有什么数据，有什么对象，有什么语法规则，这些通通都不介绍。我们怎么介绍？就是直接数据拿过来，我们要怎么用，怎么分析，第一步做什么，那我们看一下这个，那么第一步是什么？我们从第二章来，就是安装，假如我们现在有了一个数据，我们需要用r语言，我们第一个问题就是把r安装到自己的电脑上面去，如果有使用的是windows系统的话，最好把自己的用户名改成英文或者是拼音，不要用中文作为用户名，因为R语言它是英语的使用者开发的，所以说他的这个编码的话可能会对中文不是很友好。我们之前碰到过一个问题，就是当我们使用中文作用户名的时候，可能没有办法画图。 然后我们会后面会帮助大家解决一些安装中的问题，然后的话我们会介绍安装之后的各个界面的介绍。然后呢我们也会介绍如何更加方便的使用r，如果我们是使用原生态的r的话他会非常的难用，他相当于只是给我们提供了一个引擎就是做R计算的一个引擎。我们还要需要有一个写代码的一个界面，更加方便我们进行交互，那么我们会使用Rstudio，那目前的话，Rstudio应该是使用最广泛的，当然越来越多的人也使用Microsoft的那个VS code，但是我们使用R语言，他其实也是挺方便的。 然后我们也介绍他的这个各个界面以及，如何开始我们的这个数据分析，那么现在是从最基础的开始，软件的安装，然后呢告诉大家我们现在拿到一个数据之后，我们如何去把这个数据导入进去，那么在SPSS里面大家知道，就是一个File导入，那么在里面呢我们除了这种方式以外，我们有其他的方式，我们也会进行批量的导入，我们不仅要导入一个数据，我们可能需要导入多个数据，导入完数据之后呢我们就有东西了。 我们的数据进入到里面了，那么我们从现在就开始认识R里面的这些数据，就是R里面的这个对象，然后我们会以这个为基础来讲解R里面的各个对象有什么特点。 首先我们要讲解数据和处理，因为这个过程有点复杂，所以我们会分成两次来讲解。我们会首先讲解单个对象在R中如何操作，一个一个的我们在Rstuido中都是可以看到的，然后讲解一些运算规则。这时候，我们就把R的一些基础知识放在这里了。然后我们讲解每个对象的特点之后，会展开介绍一些函数和规则等等。然后我们就可以开始自己的处理工作了。我们会按照tidyverse的风格进行基本的操作，或者我们叫做比较管道的操作。在这个过程中，我们可能需要给大家演示一下。如果大家能理解了，我们就可以进入下一个阶段。但是，如果有些同学对编程没有基础的话，第四章和第五章可能会比较难理解。所以我们会多停留一段时间，确保大家能够理解。 然后我们已经导入了数据，接下来就可以进行一些探索。看一下我们数据长什么样，他有什么样的一个模式，特点，我们应该对他进行什么样的一些分析，这就是这就是去了解我们的数据。在传统的心理统计学和SPSS中，不知道老师有没有要求大家查看原始数据。实际上在我们用做数据分析的时候，我们一定要去看，并且是在开始的时多的去看原始数据他到底是什么样子。我们不能只看统计指标，我们一定要看数据他到底是什么样子，这样的话就避免让我们发生一些非常非常基本的错误。 那么这个数据探索的话其实他包括两个部分。一个部分就是我们会去给他进行描述。第二个部分就是对他进行一些基础的一些可视化，所以探索数据部分其实已经包含了我们两个部分的一个知识点：一个是描述统计，另一部分就是最粗糙的可视化。因为这个时候，我们只需要自己看就行了，我们不需要给别人看，我们也不需要去把它做的非常精美，所以这个时候是最基础的这个可视化，然后的话我们就会接下来几章我们就会告诉大家如何用r语言实现大家常用的一些统计分析，因为这个可能是我们心理学常用的。 所以我们先展示一下，然后他的这个分析的流程是什么样的。这几个部分呢，其实应该是可以并行展开的，但是我们没法进行并行展开，所以我们只能依次的介绍。 然后呢，介绍完了之后呢我们会介绍一个目前来说在国际心理学界比较流行的一个做法，或者说大家在推荐的一个做法，就是看我们的这个结果是不是稳健的，那么这个时候比如说我们同样一批数据，我们采用多个方法来分析它，最后我们得到的结论是不是一致的，这个称之为Multivese。在心理科学进展上最近有一个文章，介绍这个Multivese的。大家有兴趣话可以去看一下。 那么到这个时候的话，其实我们基本上统计分析就基本上，如果我们只做传统的这个数据分析的话，我们就其实就已经做完了。那么我们接下来就是，我们汇报结果，这个时候我们如何得到一个可发表的图像，或者可发表的是一个插图，那么这个时候我们会进一步的讲我们如何进行拼图，如何操作每一个每一个这个元素然后，如何把多个图拼到一起，让他更加的美观，以及他的比例等等。然后呢我们会讲一下，这个叫做文学编程literature program就是，实际上他就是把我们前面讲到——就是在这一章以前，我们讲代码，所有的都是直接代码，那么我们在这个地方的话我们就开始介绍如何把r代码和文字进行混合，也就是说我们同一个文档里面既有文字描述也有代码，这个代码还是可以运行的对吧，他生成的图片还可以直接插到这个文档里面。 我们就要介绍这个Rmarkdown,它是用LaTex的语法,所以我们会介绍一些最基本的LaTex的语法，帮助大家进行排版，还有公式的撰写等等，然后到这里的话基本上就是我们主流的部分，一个人干活的话基本上就差不多了。但是我们知道，现在的已经都不是一个人干活对吧？所以我们要经常跟他人进行协作，所以我们后面介绍就是如何跟合作者或者导师进行协作那么这个地方就涉及到两个，一个是版本控制——其实一个人干活的话你版本控制也很重要，为什么呢？因为你可能前后代码有很多迭代对吧，你有的时候可能删除一个功能，但是你后来发现，这个删除功能它是有用的，但是如果你直接删除的话，你就找不到了，那么如何我们能够找到以前的版本，这个其实是，当大家写代码越来越多的时候很重要的，另外一个就是多人协作对吧，通过这个Github来进行多个人进行同时，完成一个数据分析，你完成这个t test，我完成f test，最后我们两个人，进行一起合并，这样的话我们能够更加快速，更加有效的进行协调。到第14章的话，我们相当于是一个研究做完了，然后呢，我们可以计划下一个研究，这里我们会介绍一些心理学常用的方法，包括meta-analysis元分析。元分析实际上就是我们把多个研究的这个效应量进行综合，综合起来之后我们就有一个用来估计样本量的一个东西，那么我们接下来就是做这个power analysis。以及我们如何在没有任何数据的情况之下，我们就可以把自己的分析数据的代码先写出来，那么这就涉及到这个假的数据对吧，fake data或者叫做simulator data。 如果我们涉及到这个数据非常复杂的话，我们可能会考虑如何进行变形处理。就是说我们的数据量比较大的话，如果我们做模拟的话有可能他要花很长的时间。但是我们的CPU有多个核对吧，我们是不是能够把多个核都用上来，让他更加快速有效的进行这个模拟，当然这个可能是我们看情况啊，如果说有时间的话，最后大家自己去搜索就可以了，因为对于传统的行为学的这个研究来说，除非我们用贝叶斯的这个混合线性模型，要不然的话基本上都是在可预期的时间内能够完成的。然后最后的话，我们基本上完成了整个分析。我们全部做完了对吧，然后可能，假如我毕业了，那么师弟师妹他们如何来重复我的研究？或者导师如何能够重复我的研究？那我可能过了一段时间之后，这个版本有不断的更新的，那么别人如何能够还能够重复我的研究？这个时候我们如何把这个版本，这个包给记录下来发布？现在比较主流的这种计算机的方法就是容器技术。用这个帮助我们来更好的达到这个computational reproduce ability，当然这个地方我可能只会做点介绍，大家如果真的要去完成这一点的话，可能也需要花点时间去琢磨。因为R语言他有的一些包可能会涉及到比较系统里面比较底层的软件的交互，所以呢他可能——就是说依赖于你使用的什么包，如果你使用的包是比较主流的，比较常规的，那可能，你把它打到docker里面是很容易的，如果你涉及到的包可能是比较新的，有可能会他有很复杂的一个底层的一个编译的话，那你有可能会就没有那么容易打包。 所以如果我们再回过头看一下，就是说我们整个教学大纲就是按照我们一个研究生，拿到数据之后会做什么一步一步的往下走，所以我们不会说很系统的去介绍R语言里的知识，我们就是碰到了什么我们就讲什么，那么我们会用什么数据呢？我们的数据就是我之前的公开数据。这里面包括了2019年在scientific data发表了一个数据，那么它的数据呢以问卷为主，那么还有原研究，我们后面会把这个发给大家。另外一个是实验数据，也是我2020年发表的一篇文章，我们就会从这些数据开始，然后一步一步的进行处理。 那么当然如果大家说OK，我想就在这个学习的过程中，顺便把我的数据给处理了，那我就把我自己的数据拿过来可不可以呢？也是可以的，你可以尝试，然后你碰到什么错误。如果说需要问的话呢也可以问，但是不能太超纲了。 我们这里面大家可以注意到一点，就是我们对统计方法的讲解没有涉及到很深，因为我们是R语言课对吧，所以是以R语言本身的操作为主，就是比方说大家这个里面设计的很复杂，我有一个链式中介模型要试试，或者什么调节模型。如果说你碰到那种技术性的问题，我们可能不会回答因为这个完全超纲了，但你说我碰到了一个数据导入的问题，这个没有问题，我们可以解答。因为现在是说一个是知识一个是操作，我们教的主要是操作，因为实际上如果你懂了，这种比较复杂的SEM的知识的话呢，你要操作起来很简单，可能就一两行代码就能够解决了或者复杂一点就是十几行代码就能够解决了，但是，如何设置这个模型，如何解读模型的输出，这个不属于我们R课的内容，而是属于你的统计知识的内容，大家清楚了吗？所以我们这个课，你可以把自己数据拿过来，但是如果你的数据非常specific分析的话，我们可能不在我们这个课程所覆盖的范围之内，但是可以帮助大家去把前面这个部分解决。你可能原来完全不知道怎么使用R，那么我们现在就教你怎么把这个数据导入怎么使用，做基本的数据分析。 那么到后面，你可能很容易就找到适合你的这个数据分析的工具包，那么你需要去阅读这个工具包相关的知识点，去把它用于自己的数据。有时候我们会以这个原始数据——两个原始数据，一个是问卷数据，一个是反应时的数据为基础，然后一步一步的去走完整的过程，那么中间大家可能会有一些要抄写代码的地方，这个是就是大家可以在课堂上实现。那么我们可能会需要，为了保证大家能够不断的进行这个讨论和反馈，我们需要分组。那我们可以加到群里面，然后后面需要进行分组，大家小组长最好能够负责带一些插线板之类的，这样的话就保证每个人都能够有电，在上课的时候有电来进行后续的操作。 我的想法就是最开始的时候我们就分组，那么大家可以比方说根据自己的兴趣，或者是我没有兴趣的话，我们可以由助教来进行随机的分组，那么分组之后呢大家比方说碰到了问题大家相互讨论，我不知道大家以前在做数据分析的时候是不是相互讨论，但是在写代码的时候，相互讨论是一个非常有用的一个东西，为什么呢？可能你写了一段代码之后你犯了一个非常小的错误，然后导致你的代码没办法运行，你自己看不出来为什么，因为你太熟悉了这个代码，但是别人来看的话可能一眼就看出来了。 所以呢大家可能就可以形成小组，在上课的时候练习的阶段，大家就可以相互来进行这个相互进行检查，同时呢我们也有几位助教，我们分组讨论的时候呢，助教就会跟我们一起来解决。 我们现在有一个助教在在线上没来，蔡镇在这里，胡孟真在这里，田彩玉也在，还有我们这边有两个本科生同学也非常积极的愿意协助，一个是柏松石，然后一个是孙禾嘉，然后他们可能主要是不一定就是能够解决大家的问题啊，就是有些其他的问题是他们帮我负责，大家不要觉得有压力啊。本科生来当助教了是吧，太可怕了。 我们会后面会有一个我们课程的PPT，完成之后会把它上传到这个Github，为什么我有这么多助教呢？有一部分助教是会把我们上课的录音进行一个，就讲的这个东西进行一个文字的整理，然后让他更加的符合逻辑，把它变成文字稿。 有些同学或者老师，特别想学，但是好像没有时间过来，你觉得参加小组讨论也太麻烦了，后面就可以看这个文字稿。或者这个书也可以电子书也也是可以的。这大概就是我们这个课程的安排。 这个课程的整个大纲啊，相比上学期的话是进行了重大调整的，也不知道效果怎么样，大家跟我就一起一起体验。如果大家感觉很很糟糕的话，也可以继续跟我反馈，我后面在不断的改进。我们去年的结构是跟今年不一样的，去年结构就是按照传统的计算机编程语言的方法来进行讲解：先讲他的基础知识，再讲如何应用到心理学，但是讲他的知识的时候就会比较枯燥，很多同学就听着听着可能就觉得听不懂了，有可能就心情就到了谷底，坚持不下去了。今年我们希望大家尽量不要出现这种情况。在这个过程中，不断的把自己的每一个小的问题解决，如果大家看我们这个大纲的话，我们每一讲都是一个问题开始的，所以我们每一节课希望能够解决一个问题，那么在这个解决问题中的话，有的时候我PPT讲的内容不一定能够完全解决你的问题，因为你有可能在你的电脑上去解决这个问题的过程中你碰到了新的问题，那么这个时候我们有很重要就是小组讨论。我和助教来一起帮助大家解答疑惑，这个基本上就是我们这个课的安排。 2.3.2 成绩分配 那么选课的同学，可能就会涉及到这个成绩的问题了，那么我们一般来说的话就是出勤，只要大家来的话，就有10%的分数，第二个的话就是小作业，我们会给大家三个小的作业，然后帮助大家去练习课堂上的一些问题，那么小的作业的话最好大家是单独完成，或者是通过小组讨论之后，自己每个人提交一个代码。 最后的话会有一个大的作业，我们一开始不就分组了吗，每个人会需要选用比方说公开数据或者我们给的数据，做一个完整的这个数据分析，然后生成一个PDF，并且能够在课堂上进行汇报。最后的话，大家需要提交的作业就是一个r代码文件.rmd和一个生成的PDF文件。 那么我们会检查这个代码文件是不是真的能够生成这个PDF，如果生成不了的话那可能会稍微扣一点分，然后还有就是大家需要以小组的形式汇报一次，这是大作业。 2.4 如何学好这门课 如何学好这门课？我觉得最关键的就是不要害怕这个课程，这课程其实没有大家想象的那么难，为什么呢？因为我们不需要成为r开发者，我们也不需要懂很多很多r里面代码，我们只需要成为一个合格的调包侠就可以了。别人开发的包我们能够合理的使用，这就是我们这门课的目的，大家如果说有志于要成为大神，比方说我要开发某一个包，或者是我要后面所有的东西都用R解决，包括给自己建一个网站什么的，这个东西不在我们的这个课程范围之内，也说大家能够用r语言，第一个就是消除r语言的畏难心理。能够用r的这个生态里面的一些包帮助大家解决问题，这就是我们这门课要达到的一个，让大家入门的一个目标那么当然我们也希望通过这个入门让更多的人，能够成为长期的这个R语言的使用者，能够长期的在这个community里面活跃。甚至有一天能够为他人答疑解惑，或者是明年的时候来给我当助教。 那么我们这个r语言，虽然说不直接讲统计知识，但是还是会涉及到一些统计知识的，那比方说我们讲的主要是操作，那么一个这个函数或者是一个代码，他的运算出来这个结果如何解读，其实需要一些统计知识的，如果你完全不懂的话，那你肯定是很困难的。 第二个就是敢于尝试，一个就是说大家一定要去不断的犯错，另外一个就是说你可以借助一些比较新的一些东西来去帮助你解决这个问题，我们有些助教就是,非常熟练的使用ChatGPT 。假如你没办法使用ChatGpt，是不是还可以使用Bing的相关功能？它也非常的强大。我们最好是用英文进行搜索，因为相对而言，英文的这个社区要比中文的社区要强大很多，把你的问题用英文描述出来，然后在Bing里面搜索，99.9%的这个情况之下你都能够找到答案，如果你的问题描述是正确的话。你也可以很简单就把那个报错最关键的地方放到这个搜索框里面，基本上你也能够找到答案。R语言他再怎么统计，他也本质上也是一门计算机的编程语所以他还是有编程的这些成分在里面，那么编程他最大的一个特点就是你要需要去勇敢的尝试，不断的犯错，犯的错越多的话，学习的越多，尤其在课上。假如说你犯了200个错误300个错误，那么你后面自己分析数据的时候可能还是会碰到这些错误，但你就知道怎么解决了。所以只有多犯错才能多学习。 然后的话，我们需要以计算机的这种思维方式思考，就是计算机他是非常非常机械的，你告诉他什么，你输入什么指令，他就给你什么结果。所以如果出了错误，一定是我们的指令或者是哪个地方出错了，所以很多时候你需要把这个逻辑想清楚。特别是你有一个比较复杂的分析问题，你要想我第一步做什么，第二步做什么，第一步的这个输入变量是什么，输出变量是什么，这个输出的变量，他能不能进入到下一步作为输入。简单的这种机械的操作的思考，能够帮助我们去使用这个R语言。 然后第三个当然就是我们在小组的讨论中一定要相互的帮助，我有一些朋友，他是学习计算机本科的，他们的快乐之一就是上课的时候，相互debug，相互帮助，因为写代码写多了之后你会发现他是一个非常有及时反馈的一个事情，你输入一行代码，他给你一个正确反应非常开心，输一行代码错了，能够解决了也非常开心。但有一种情况对于初学者来说，就是你犯了一个错误，结果你两三天解决不了，就非常的头疼。那这种情况的话，如果是有人能够帮你的话呢，就能够其实极大的促进正反馈。还有一个就是，遇到比较复杂的代码的话他确实有可能起作用了，但是你可能也不知道为什么，他不起作用你也不知道为什么，但是我们需要尽量减少这种情况。 其实当你越来越熟悉某一个编程软件之后，大部分的错误你都是知道为什么，那不知道大家有什么问题吗？ 我去年给大家说的时候,会有一个很强烈的情绪的变化,就是学r的基础知识会慢慢情绪会变得很差,因为你发现自己又听不懂,然后经常犯错。今年的话我希望大家稍微平缓一点，这样那种不断波动的一个状态比较好。如果没有问题的话我们这节课就到这里。 "],["lesson-2.html", "3 第二讲：如何开始使用R 3.1 要解决的数据分析问题简介 3.2 如何安装 3.3 如何方便使用？Rstudio的安装与界面介绍", " 3 第二讲：如何开始使用R 前言 数据分析的出发点是解决问题。也就是说，我们数据分析的过程都应该是问题导向的。因此，数据分析中的一个关键在于明确地知道自己要解决什么问题，要有问题的意识，这对科研工作者来说尤为重要。学习R语言或其他计算机语言，或进行一系列的数据分析，其目的都是为了回答一个特定的问题。问题的重要性本身最终决定了数据分析的价值。 问题可以是科研中的理论问题，也可以是现实生活中的实践问题。例如，交通分流、道路设计、产品质量等的分析。实践问题也可以是收集证据来辅助决策。比如，在新冠疫情期间，行为科学(behavioral science)在公共决策中的作用受到了重视，通过大量的行为数据，可以帮助政府或决策者进行更好地决策。因此，数据分析的出发点是解决问题，这也是学习数据分析中首先需要明确的。让数据分析变得严谨和可重复，是为了更好地解决问题。我们不能忽略这个最根本的出发点。 3.1 要解决的数据分析问题简介 3.1.1 第一个数据: 人类企鹅计划数据 为了帮助大家更好地学习如何用R语言解决问题，我们提供了两个示例问题。这两个问题是我本人先前的研究中遇到的，因此心理学背景的读者可能会比较熟悉的。 第一个问题是有关人类社会关系和体温调节之间的关系。我在读博期间参与了Hans IJzerman博士的这个项目。他关心的问题是:人类的体温和社会关系的关系。从演化的角度讲，哺乳动物的生存需要苛刻的环境：身体的核心温度必须要维持在很窄的范围之内，哺乳动物才能够生存。作为哺乳动物，人类要在相对恒温的条件下才能生存。生物学的研究发现，哺乳动物会形成一个社群，通过群体来帮助群体中的每个个体调节体温。Hans当时感兴趣的一个问题是：人类身上是否也存在这种机制？人们的社会网络本身，是否能够会帮助我们去体温。Hans提出了自己的理论假设(IJzerman &amp; Hogerzeil, 2018)，并想验证社交网络是否能够调节体温，于是进行了一项大规模的跨国实验，收集了来自十几个国家的数据。 他首先进行了一个预实验，然后收集了跨国的数据。这个数据集来自12个国家，包含了1,500多个人的信息，其中包括多个问卷，例如，怀旧程度、对家的依恋、主观压力、新陈代和社交网络质量等。这个数据集最初是通过Qualtrics在线收集，现在已经公开可以使用(Hu et al., 2019, sci data)。 在这门课程中，我们会假设自己是Hans课题组的成员，思考如何进行后续的数据分析。这个过程从数据导入开始，一步一步进行。我们最终可能想要解决的问题是如何预测核心体温，其中一个重要的分析方法是有监督的机器学习(条件随机森林, conditional random forest)。我们将寻找能够预测核心体温的变量。在下图中，红色线以上的变量对预测核心体温有很强的相关性，红色线以下的变量预测效果较弱。换句话说，虽然我们测量了很多变量，但通过机器学习算法，可发现只有部分变量与核心体温有较强的相关：complex social integration (CSI)这个指标。CSI是社交网络的一个指标。在Hans的研究中，也使用了一些传统的问卷分析方法。比如，调节分析。他发现是否处于亲密关系对CSI与体温之间的关系是有调节作用的。 要进行整套分析的研究，首先需要将原始数据输入到R语言中，然后进行数据清理和描述性统计，例如数据质量、问卷信度、均值、标准差等统计指标。此外，也可以进行探索性的数据分析，例如相关矩阵，来探索变量之间的关系。最后，我们需要呈现研究结果，包括condition random forest的结果和mediation的结果，并将其整合到可视化报告中。原研究实际上并没有将数据分析和结果报告整合到一个完整的RMD文件中，而是采用传统的方式来准备手稿。在我们的课程中，将使用Rmarkdown来完成整个数据分析，并生成PDF文稿。 我们这里的这个数据分享流程可能只是一个大致的流程，里面可能还会有一些小的细节，例如如何去假定变量之间的关系，当然我们进行的是别人研究的复现，通过复现去了解R语言数据分析中的流程，提高自己的技术。 3.1.2 第二个数据: 知觉匹配任务数据 这个实验是一项简单的认知实验。在认知心理学中，我们通常会在实验室中进行，让被试进行一些简单的按键反应。这个任务可以分为几个阶段。首先是学习阶段。在这个阶段，我们会呈现几何图形（正方形、三角形和圆形）和三个人物标签（好人、普通人和坏人)，然后让被试在图形与人物标签之间建立联系，例如将三角形和好人联系起来。接着，我们会在电脑屏幕上呈现一个图形和一个标签，被试需要判断屏幕上呈现的图形和人物标签是否匹配。这是一个非常简单的任务，通过被试练习了二三十次后，就能够熟练地完成这个任务。 接下来，我们会让被试完成两个任务：匹配任务和分类任务。匹配任务需要做出一个决策，即图形和标签是否匹配。它的反应窗口非常短，大约在800到1100毫秒之间。分类任务需要判断图形是好人还是坏人，然后按相应的键。这个任务也需要在非常快速的时间内进行反应。 我们的实验设计是一个 2*2的被试内实验设计，自变量包括人物标签（自我 vs 他人）和效价（valence）。 在分类任务中，采用两种分类的标准：自我vs他人，好人vs坏人。在学习过程中，记住了四个人物标签和四个几何图形之间的配对关系，并训练匹配任务和分类任务。分类任务包括按身份和效价进行分类。 在研究中，我们想要了解即时学习到的社会意义是否会影响对几何图形的反应。我们发现，给几何图形打上社会标签后，会影响到反应时间。虽然总体上来说，好的自我反应最快，好的他人的反应也不错，但是坏的自我和他人都比较慢。这是一个总体的趋势，但并不是每个几何图形都是这样的。我们使用这种方式来绘制图形，既能看到总体水平上的结果，也能看到个体差异。这可以避免我们的过度推断。 上图右边是d prime，是信号检测论中的一个指标在心理物理学中，信号检测论是一种常用的数据分析方法，用于认知心理学研究。它可以计算d prime，即敏感性，这比准确率更能判断信号是否敏感。在匹配的任务中，我们将匹配条件视为信号，不匹配条件视为噪音，以此来计算信号检测论。对于非匹配任务，如分类任务，我们也可以计算类似的标准和数据。我们还使用了一个名为Drift Diffusion Model的计算模型来分析数据，它是用Python工具包HDDM完成的，但也可以在R中使用。在这个数据中，导入数据可能有些复杂，因为每个被试的数据都有三个文件，需要合并和清理数据。在数据分析部分，我们进行数据清理和可视化。 如果使用传统的方式，我们需要使用Excel进行数据预处理，使用SPSS进行统计分析，使用Excel或PS进行图形绘制和美化，最后使用Word文档进行写作。如果使用R语言，我们可以在R语言中完成所有工作，从Tidyverse开始进行数据清洗和工具分析，使用GGPlot2进行图形绘制。我们将使用Markdown或papaja来输出。在BruceR中，我们特别关注T-test、方差分析和多重比较等内容。由于BruceR对心理学数据分析进行优化，因此非常方便。正如我们之前提到的，使用R进行分析可以保留所有分析过程，并且可以通过代码直接重复分析。分析的代码和方法也非常灵活，新的方法也容易共享。 3.2 如何安装 首先是安装过程，可以在必应(bing.com)中搜索R语言官方网站，然后在该网站中下载安装程序。该程序适用于不同的操作系统和版本。对于Windows系统而言，安装过程比较简单。但对于Mac系统，安装过程可能会稍微复杂一些。Mac系统可能会有两个版本，分别为AMD64和apple芯片版本，你可能需要下载对应的包。 使用中文语言安装R语言会更加方便，可以避免编码问题。下载完成后，运行安装程序。在安装过程中，你可以选择是否自定义一些安装选项，但是默认选项通常已经足够了。为避免出现问题，请确保选择英文路径作为安装目录。完成安装之后，你可以开始实际学习操作。在此我也想提醒大家不要把安装目录放在中文文件夹里，避免中文路径可能会出现的编码问题。 在上图你可以看到，此处默认选择了中文作为系统显示语言，主要是由于系统语言为中文，安装时自动采用中文显示。安装时可能看到了一些警告(warnings)，猜测是语言设置引起的。在R语言中也会遇到这种情况。这里有两种类型的警告：一种是称为”警告（warnings）“，另一种是”错误（errors）“。对于那些被警告的代码，我们需要仔细检查，看看是否会对我们的运行造成严重的影响。如果没有明显的影响，那么就可以忽略。 我们可能会遇到一些问题，例如，遇到一些非UTF-8编码的语言编码。这意味着我们不是使用Unicode编码方式。为了解决这些问题，我们通常使用UTF-8编码方式，这是中文中非常常见的编码方式。在大部分计算机系统中，我们都可以使用UTF-8编码方式，因为它是国际上通用的编码方式。 3.3 如何方便使用？Rstudio的安装与界面介绍 大家都已经成功安装了R语言，现在看到的是console控制台界面。早期的R语言使用这个控制台进行输入和操作。比如，我们可以输入一个简单的命令，例如： a &lt;- rnorm(100) plot(a) 这个命令将在变量a中存放了100个数据。控制台支持用户输入任何语句，比如demo()，plot(x,y)。（这将自动打开一个窗口显示绘制的图形。） 对于没有编程经验的同学来说，控制台可能是一个不太熟悉的点。为什么我们在这里输入这个东西呢？为什么会跳出这个东西呢？有时候会出错吗？例如，我们输入a会得到一个数字，但是如果输入b，他会告诉你没有这个对象。但如果输入c，又有不同的结果。 这里的一个难点是，为什么有些输入会被计算机执行，但是有些不会？还有，为什么有些输入会产生图形效果，有些则不会？ 这正是我们写代码与计算机交互的方式。这种方式与图形界面的软件（如SPSS）有所不同。在使用SPSS软件时，我们通过点击屏幕来进行交互，并从菜单中选择选项。然而，当我们使用R语言或其他语言时，我们主要使用的不是这种图形界面，而是只有一个输入代码的窗口，通过在这里输入代码与计算机交互。对于其他语言也是一样。 这样的交互方式的缺点是，我们不知道哪些变量保留在内存中。当我们进行一系列操作时，我们不知道哪些操作是我们刚才输入的。它们存在于计算机的缓存之中。如果我们输入了许多变量，如a、b、c、d等，我们可无法回忆起这些变量的细节。因此，当我们学习R语言时，我们希望有一个更好的地方来编写代码。我们不希望一次只写一行代码，然后再运行。因此，我们需要一个更友好的代码编辑器，而不仅仅是控制台。虽然控制台是与R进行交互的窗口，但有时不够友好。 3.3.1 Rstudio的安装与界面介绍 幸运的是，我们可以使用RStudio来完成这项任务。所以我们需要安装好R之后，再安装RStudio。有两个版本可以选择：Windows版和Mac版。安装时，我们需要将其安装在非中文目录下。安装后，我们可以选择64位系统。使用RStudio的最大好处是我们可以一次编写一长串代码，然后执行它们。 打开RStudio后，显示出的是一个白色背景的界面，我们可以通过更改外观中的主题来改变界面颜色。在这个地方，工具中有一个全局选项，外观，你可以把它改成更专业的样式。现在界面变得专业很多了，这是我们通过调节一些参数实现的。当然，也可以根据自己的喜好来调整。另外，将界面背景改成灰色可以省些电，因为不用一直发亮光。 接下来讲讲界面的调整。我们的界面是由四个面板组成的，但是在刚安装RStudio时，我们看到的是三个面板的界面，界面可能还有点不一样。我们可以新建一个project来呈现一个新的面板。 现在我们有4个窗口。这4个窗口分别是脚本编辑区、控制台、环境和文件。 脚本编辑区 脚本编辑区可以记录下我们想要写的代码，并且可以选择性地运行。我们也可以直接创建一个R脚本，然后输入代码。在输入代码的时候，RStudio会提示我们，根据我们之前输入的字符来预测我们可能想要输入的代码。这样的话，我们就不用记住很多代码，可以更方便地完成代码。我们可以将所有的代码放到一起，形成一个脚本文件，一般以R结尾。我们可以用RStudio打开它，这样可以更好地编写R代码。 右上角这个地方，它实际上它不是一个窗口，而是多个窗口，第一个选项卡，这是一个被称作environment的窗口，类似于人的工作记忆一样，会存储我们运行中的所有的数据和变量。现在这块区域是空白的，因为我们没有进行过任何的数据的操作和读取。如果我们运行了一些有关变量的R指令，涉及到的变量就会在这块区域中出现。 第二个窗口是history,我们所有运行过的代码会在这里被列出，就像我们刚才输入的指令，都会保存在里面。 控制台 控制台界面 Console，即控制台，展示了所有程序的交互结果。代码的运行结果与报错都会在该窗口展示。在Rstudio中不仅有Console控制台，还有一个teminal，这是一个与windows系统进行交互的界面。在Mac OS中也有相同的Termianl终端。我们可以让它进行在后台安装软件之类的工作，不过我们使用的不多。 在后面我们使用github的时候，我们可能会较多的涉及到Teminal相关的内容。 右下角窗口集 右下角有许多子窗口，我们一般会频繁地用到其中的两三个，一个是files，一个是plot，可能还有一个help。 当我们对某个指令的具体功能和提供的参数不是很了解的时候，我们可以在help中进行输入和搜索，它就会提供给我们一些相应的解释和说明，我们可以详细的了解相关的函数和包。 file窗口是我们的文件浏览器，我们可以看到我们打开的文件夹里有什么文件。并且可以进行打开。因为我们打开的是一个R project文件，它会自动将工作目录关联起来。 plot界面是我们画出来的图的展示的地方，我们使用plot等指令绘制的图表将会在这个界面中呈现。我们可以通过手动拖拽窗口的大小来调整输出图片的大小，并且可以进行保存等操作。 除此之外，我们还有packges选项卡，在其中我们能看到我们安装的包，并且对其进行管理。 3.3.2 测试Rstudio 现在，我们可以对我们的安装过程进行一个检查，如果我们能够在Rstudio中正常的输出这样的图表的话，就说明我们的安装基本上没有问题了。 image-20230309144756488 典型问题 由于大家的输入都是以中文为主，我们是中文用户，所以输入法很多时候会保持在一个中文输入的状态，但是这个时候你的标点符号也会是中文的，但是所有R的指令需要的都是英文的标点符号，如果你输错了，可能就会产生问题。比如产生unexpected input这样的报错 3.3.3 R语言中的包 package是R中非常重要的一个概念，我们在前面提到过package窗口，它管理的对象就是我们的包。 Packages（包）是一种模块化的方式，用于扩展R的基本功能。Package包含了函数、数据集、文档和其他需要扩展R的元素，这些元素按照特定的结构和命名方式进行组织和存储，方便用户引用和使用。它能够以类似R的方法对R的功能进行扩展。有了各种各样的包，才能让R语言的生态变得丰富。 包里一般会包含一些有特定功能的函数，有些也包括一些数据集，能够让你去方便的测试它的一些函数。也包含了说明文档，来解释包和各个函数的功能。 3.3.3.1 包的分类 R包的基本分类大概包括：Base:（安装R时自带的包）和其他由社区所贡献的、内容丰富、领域特异性（数据可视化、统计分析、机器学习、深度学习、自然语言处理、图像处理）的包。有些相关的包会朝着同一个风格进行开发，形成一个系统，例如tidyverse; easystats。他们有一个自己特殊的语法风格。 "],["lesson-3.html", "4 第三讲：如何导入数据 4.1 路径与工作目录 4.2 读取数据 4.3 了解R里的数据 （R语言中的对象）", " 4 第三讲：如何导入数据 4.1 路径与工作目录 那么什么是工作目录呢？简单来说，工作目录就是R会在其中寻找文件的文件夹。如果你没有指定工作目录，那么R会默认使用你的用户文件夹作为工作目录。但是，如果你的数据文件不在用户文件夹中，那么你需要告诉R去哪里找这些文件。这就是为什么我们需要设置工作目录的原因。我们可以使用“get.wd”命令来查看当前的工作目录。当我们打开一个R项目时，它会自动打开项目所在的目录。 4.1.1 相对路径与绝对路径 建议为每一个数据分析项目设立一个文件夹，并且在其中创建一个R项目。这样，每次打开项目时，它都会自动打开所在的位置。在讲解路径和工作目录的概念时，我们需要先了解两个概念：绝对路径和相对路径。绝对路径指的是文件或文件夹在硬盘上的真实路径，而相对路径则是指相对于当前路径的位置。例如，在chapter3文件夹中，data文件夹的相对路径为./data。相对路径的好处在于，当我们复制整个文件夹时，相对路径的位置不会改变。相对路径是相对于当前路径的位置，而绝对路径则是相对于硬盘的起始位置。 如果我们有一个子文件夹叫做data，那么它就是当前文件夹的子文件夹，也可以称为子目录，而当前文件夹则是它的父文件夹，也可以称为父目录。如果我们复制这个文件夹到另一个电脑上，它们之间的相对关系不会改变，但如果我们把它从D盘复制到E盘，那么绝对路径就会改变，相对路径仍然保持不变。因此，我们建议把所有文件都放在一个文件夹中，这样所有文件的相对路径都是相对于这个文件夹的，方便编写代码。如果文件夹放在不同的位置，每次引用或查找文件时都需要使用绝对路径，非常麻烦。 在Windows系统中，每个磁盘都有自己的盘符，例如C、D、E、F盘。绝对路径是从盘符开始一直写到当前文件夹，对于Mac和Linux系统，它们只有一个根目录，没有盘符的概念。我们最常用的是“用户”，比如我的用户名，然后有一个“主目录”，这就是用户的文件夹。当我们使用苹果系统或者Linux系统时，基本上很多时候都是基于这个“用户”文件夹或者“users.html4c”进行操作。除了“主目录”以外，它可能还有一个“下载”，就是你下载的文件都放在哪里，还可能有一个“图片”，就是你的各种图等等。所以它就是在一个文件夹下面有不同的子文件夹，比如有“etc”、“用户”还有其他的“根目录”等等。我们主要就是基于“用户”，然后建立自己的一系列的文件。在Windows中，它可以分成三个盘，每个区都是自己的盘符，然后基于这个盘符你又可以重新定义一个新的路径。这就相当于路径的定义不太一样。我们刚刚也讲了相对路径和绝对路径。如果我们所有的分析都放在一起的话，那么我们用相对路径是更好的。就像我们写代码的时候，不要把C盘D盘什么这些一整串写进来，因为写进来之后你换一个电脑，这个C盘它就不是C盘了，它可能是变成D盘了，或者你换到苹果上面之后，它可能就没有C盘了，它就是“用户”开头了。 4.1.2 Here包的使用 介绍一个常用的包，包名叫做Here。Here这个包可以帮助你很快地去定位你的当前的新的目录。打开了这个，就能够迅速地看到我自己在哪。它另外一个好处就是我们能够不用考虑我的系统是什么，然后我就直接能够把它的一系列文件夹都调出来。 我们的Windows系统中的地址栏和R中的地址栏斜线方向不同。如果我们直接复制Windows地址栏的地址作为绝对路径，可能会出现错误。为了避免手动拼接路径出错，我们可以使用here函数来自动补全路径。这个函数会自动将当前文件夹的路径补全，我们只需要在路径中加入文件夹名即可。例如，我们可以使用here(“data”, “match”)来获取data文件夹下的match文件。这个小技巧可以帮助我们避免一些错误，特别是在不同电脑之间切换时。 4.2 读取数据 工作目录是R与我们的硬盘交互的接口，我们可以使用getwd()函数来获取当前工作目录的绝对路径。我们可以使用相对路径来引用子文件夹的位置。接下来我想要查看数据的相对路径和绝对路径。假设我们的工作目录是在桌面上，数据存储在”data”文件夹中，其中包含两个子文件夹，一个是”match”，包含178个文件，另一个是”pengiun”，只有一个文件。相对路径中的一个点(.)代表当前文件夹，数据在”data”文件夹中，“match”文件夹下有178个文件。如果我们要读取某个文件，必须完整地写出文件名。对于”pengiun”文件夹，它的相对路径是”./data/pengiun” 4.2.1 read.csv函数 现在我们知道了数据的相对路径，可以使用”read.csv”函数来读取数据。我们需要给数据一个名称，例如”pengiun.data”，并使用”read.csv”函数来读取数据。在函数中，我们需要指定相对路径。这里建议使用相对路径而不是绝对路径，因为绝对路径需要每次更改代码。 “head = TRUE”是函数中的一个参数，用于指定文件是否包含列名。如果文件的第一行是列名，则将其设置为”TRUE”，否则为”FALSE”。在R语言中，假用false表示，真用true表示。读取这样的txt文件时，需要使用分隔符来分隔不同列的数据。为了更直观地了解这个过程，我们可以打开csv文件。虽然这是一个csv文件，但我们也可以使用txt打开，如果我们使用文本编辑器打开，数据看起来会很混乱，因为它包含换行符。 当使用函数读取时，行是自动识别的来识别的，而列必须使用分隔符来识别。在这里，我们使用逗号作为分隔符。这样，我们就可以将数据读取为行和列。我们可以使用data table包来方便地展示读取后的数据。为了方便在PPT中展示数据，我们可以使用head函数来大致的查看数据。我们已经读取了数据，按照逗号分隔后，第一行是列名，我们可以将其识别为表头。从第二行开始是数据，第一个列的第一个值是1975，后面是1995等等。这是一个问卷数据，每个编号代表一个问卷和其items。我们可以用不同的方式对其进行编号，比如uncertainty等等。 对于常用的数据文件，我们可以使用here来定义相对路径，然后使用import来读取数据。读取后，我们可以看到有247个变量。对于.out文件，我们可以使用read.csv来读取，但是需要将separator参数设置为斜杠加t，表示以tab为分隔符。在进行统计分析时，我们需要明确定义用于分割数据的符号。除了逗号和tab键，还可以使用分号、空格或分行符等多种方式。 read.csv是最常用的数据读取函数，但实际上它是read.table的一个特殊形式。因此，我们也可以使用read.table来读取数据。但是，由于.out文件是我们自定义的一个txt文件，无法使用bruce-r的import函数进行导入。这种情况很常见，你需要清楚你的数据结构，包括分隔符和编写模式，以便正确读取数据。 完成这些操作后，R中的环境environment应该发生了变化。在右上角的环境中，我们可以看到两个相应的数据match data和PenguinData。PenguinData是一个符号，我们可以用任何其他符合命名规则的名称来代替它，比如abcd，比如xyz。这里存储了一些可操作和创建的对象，我们称之为R对象。现在我们导入的对象的种类被称之为数据框，有关数据框的知识将在之后讲到。 4.2.2 命名 在命名时，应尽量简短，避免使用中文。如果发现文件目录没有问题，但仍然出现错误，有两种可能性。一种是拼写错误，例如拼写单词penguin时少了一个字母或交换了两个字母的位置。另一种可能是文件夹的大小写不匹配，例如使用大写字母开头的文件夹，但在输入路径时使用小写字母。在这种情况下，我们需要仔细检查并逐行对比当前工作目录和文件夹的实际目录是否匹配。最有效的方法是直接点击文件的属性，查看其目录和名称是否与输入的完全相同。 4.3 了解R里的数据 （R语言中的对象） data structure R语言中的几个常用对象包括：向量、矩阵、数组、数据框和列表。其中，向量是一列数字或其他相同类型的元素，而矩阵是一个二维的数据结构，有长和宽，可以看作是将向量按行排列而成的。数组是在矩阵的基础上再往上拓展，变成了一个三维的数据结构。 在R语言中，我们更多地使用数据框，它看起来像矩阵，但每个列可以是不同类型的元素。而列表则更加复杂，每个元素都可以是不同类型的对象。这些对象在编程软件中非常常用，比如Python和Matlab。 我们没有时间讲解数据框，但是我们已经解决了读取数据的问题。我们下节课再深入讲解这些知识。在下节课之前，大家可以查看王敏杰老师和张金星老师的两本书，这些书将更清楚地介绍这些基础知识。 "],["lesson-4.html", "5 第四讲：如何清理数据—R语言编程基础 5.1 R对象的操控 5.2 函数 5.3 常用函数", " 5 第四讲：如何清理数据—R语言编程基础 前言 大家解压好我们发在群里的压缩文件，在里面，有几个文件对大家都是有用的。第一个是Rproject，打开它后会自动启动相应的路径，这是上一节课讲过的。有了这个Rproject，我们就不需要再去寻找文件，只要把所有东西都放在这里面，所有的读取和保存文件都可以通过相对路径来解决。在这里面，有一个chap4的网页html文件，它实际上就是我们今晚的PPT。还有一个叫做RMD的文件，里面有代码。大家可以打开它，可能跟我看到的现在看到的应该是一个类似的界面。在右下角的窗口里，有一个chap4 RMD，里面都是关于代码的。大家可能有一些地方不需要自己去抄代码，可以去找一些相关的代码。因为之前有同学会抄PPT上的代码，这样很慢。在这个地方，我们读取数据的代码都在这里面。在自己电脑操作的时候，可以直接点开这个RMD，这样就可以直接进行复制粘贴来完成运行，因为都是相对路径，所以实际上是比较容易，基本上不会出错。 mulu 此外，我们还有一个小技巧。在上次课上，我们的助教发现大家都直接在控制台里面写代码，这会导致代码难以查找错误。因此，我们可以使用一个叫做Rscript的脚本来记录我们所有的代码。如何创建Rscript呢？很简单，只需要在RStudio中点击“文件”，然后选择“新建文件”。这个Rscript实际上是一个txt文件，但我们可以在其中记录代码。这将在调试时非常方便。 scrip 5.1 R对象的操控 好，那么我们上次课花了一节课帮助大家解决了很小的几个问题，就是知道什么是工作路径、什么是工作目录以及什么是目录、相对路径和绝对路径。然后在理解了这些概念之后，我们第一次读取了一个数据。 这节课我们讲一下在R里面，它到底有哪些数据类型，我们如何得到进行操作。我们上节课很快地讲完了R的数据类型，但大家当时可能听得很懵。如果我们按照讲R语言的方式，它会一直都是很懵。为什么呢？因为它跟我们的数据完全是脱节的。所以这节课我们通过对数据进行操作，进行数据筛选的学习。上次课我们已经读取过数据，这次我们将按照相同的方式再次读取数据。 我们将使用read.csv来读取三个数据，并将它们合并。在这里，我们有三个数据文件，分别是707304、7305和7306。我们的分隔符与上次不同，我们使用空格而不是制表符。在空格中间，有两个引号（““）。我们这次的数据分割方法与上节课不同。上节课我们使用了斜杠和t作为分割符号，但是我们发现使用tab作为分割符号时会出现问题，无法真正进行分割。因此我们进行了尝试，发现使用空格作为分割符号更为合适，空格中间有两个引号，不加任何东西。这与上节课不同。 #实验数据参考读取方法，这里我们随便选择三个被试： match.data1 &lt;- read.csv(&quot;./data/match/data_exp7_rep_match_7304.out&quot;, header = TRUE, sep = &quot;&quot;, stringsAsFactors = FALSE) match.data2 &lt;- read.csv(&quot;./data/match/data_exp7_rep_match_7305.out&quot;, header = TRUE, sep = &quot;&quot;, stringsAsFactors = FALSE) match.data3 &lt;- read.csv(&quot;./data/match/data_exp7_rep_match_7306.out&quot;, header = TRUE, sep = &quot;&quot;, stringsAsFactors = FALSE) 我们将每个被试的数据读取到单独的变量名下面。这个符号（&lt;-）是R语言中的一个复制符号，我们通过读取.csv和.out文件，将它们复制到一个变量中，比如match data 1。因为有三个被试，我们将它们变成三个数据。然后我们使用rbind函数，将不同的数据框进行合并，合并它们的列。这个函数很简单，我们有一个新的变量名match_data.all，然后用rbind函数将所有的数据都放到里面去，包括match data 1、match data 2和match data 3。我们刚才的操作就是从这里开始的，先读取单个被试的数据，然后再将它们合并。 #组合三个数据： match.data.all &lt;- rbind(match.data1, match.data2, match.data3) 我们涉及到了哪些R语言的特性呢？第一个是赋值符号，我们之前已经讲过了。为什么我们需要赋值符号呢？因为它表示将一个值附到一个变量名上，比如将10复制到object中。我们可以对它进行操作。如果我们在object后面加上2，输出就是10 + 2，但是object本身没有变。因此，每当要保存某一个结果的时候，我们一定要把它复制到某一个特定的变量里面去。相当于说，object 只是一个标签，它下面的内容会根据我们的赋值的改变而改变。 object &lt;- 10 object ## [1] 10 object + 2 ## [1] 12 object ## [1] 10 在R中，变量名由字母、数字和两个特定的符号（下划线和点）组成，其他符号是不被接受的。我们也不推荐使用中文，因为在不同的电脑上可能会出现编码不一致的问题。为了避免这种情况，我们尽量使用拼音。关于变量命名的可读性，我们应该让变量名称易于理解，一看就知道它代表什么。例如，在我们的课程中，我们有两个数据，一个是match data，另一个是human penguin project的数据，我们可以将它们分别命名为match data和penguin data，这样我们就能够一看到这个名字就知道它代表的是哪个实验的数据。这是关于R对象的复制和变量命名的基础知识。 5.2 函数 刚刚我们使用了R-bind函数将三个对象合并成一个，这涉及到函数的概念，我们在前面已经讲过。数学中的函数实际上就是一种方法，只不过是用代码来实现。数学函数可以表示为y=f(x)，或者其他形式，其中括号内是我们输入的变量，它会返回一个结果。在R语言中，函数也基本上是这个意思，就是有一个特定的名称，比如在这里就叫做rbind，然后有一个括号，你在这个括号内输入内容之后，它就会返回一个结果。 函数是用于执行特定任务或计算的代码块，它接受输入参数，执行特定的操作，然后返回结果。函数来源有很多，安装R时，我们一般会安装R-base，这是R开发团队放在R基础包中的所有东西。这个base中已经包含了很多用于统计分析的函数。另外还有R的包，就是我们自己额外安装的包，比如Tidyverse、GGPlot、Nymer、Lmer、Lme4等。还有其他的R源，比如Cray、Microsoft、Stand等。除了这些，还可以在GitHub上直接安装一些包。虽然函数可以自己编写，但这比较复杂，我们会在后面再讲解。 我们经常需要重复执行某个任务，这时可以将其封装成一个函数，以后只需要调用这个函数即可，无需重复编写所有操作。函数是一个重要的知识点，可以用非常简单的两三行代码实现。但是有时候，当我们使用别人提供的函数时，可能不知道它的作用、输入规则和选项。在RStudio中，我们可以通过在函数名前加一个英文问号来查看它的用法。例如，在使用here包时，我们可以在RStudio的右下角看到帮助文件。 qiuzhu2 但是有些帮助文件可能并不是很有用，因为开发者有时会有所谓的“知识诅咒”，认为别人都和他一样有基本知识和内容。这时，我们可以在搜索引擎中搜索关键词和R包名，以便找到我们需要的信息。在调用已有的函数 时，我们需要先使用library加载包，然后直接使用函数名和小括号。如果没有加载包，我们可以直接使用R包名和两个冒号加上函数名来调用。个人偏好是将包名加上，因为R的生态系统非常复杂，有很多包都有类似的功能，这样可以避免混淆。 #给出R包的调用： library(&quot;here&quot;)#加载here包，install的R包需要library后使用 ## Warning: package &#39;here&#39; was built under R version 4.2.3 ## here() starts at C:/GitHub/R4PsyBook/bookdown_files/Books/Book here::here()#这里第一个here是R包，第二个here是调用这个包里here函数 ## [1] &quot;C:/GitHub/R4PsyBook/bookdown_files/Books/Book&quot; 5.3 常用函数 我们刚才使用了R中非常常用的函数R bind。现在我们先停一会儿，让大家完成刚才的操作，即通过Match Data完成数据的读取和合并。这个操作在真实的数据中非常常用，当你有不同数据文件或不同数据来源时，你肯定要对它们进行合并。现在我们练习读取三个不同base的数据，将它们合并成为一个数据，并体验一下R bind函数的使用。然后我们再讲下一个知识点。如果大家按照压缩包来打开，应该会很快。我刚才有一个有趣的问题，就是现在Windows系统比较发达，它可以直接读取zip文件，没有解压的过程。但是这个时候你发现，打开R以后，里面的文件是不完整的。所以一定要把那个zip文件右击然后提取成为一个完整的文件夹，这样我才能看到全部的内容。 5.3.1 unique() 我们想看一下这个数据有没有问题。首先，我们想查看这个数据里面的实验条件是不是完整的。如果我们直接看，我们可以点开右边的Match all，然后查看Label这个实验条件。我们想看这个实验条件是不是只有几个独特的值。如果我们的实验条件就是6种条件 的话，那么它就只应该有6个值。这种情况下，我们不可能一个一个地去数。在R语言中，我们可以使用一些简单的函数来查看某一列的唯一数值有多少个。首先，我们可以使用head函数来查看数字的前几行长什么样。然后，我们可以使用unique函数来查看这个列中有多少个唯一的数值。我们可以使用美元符号和列名来选择完整的列。如果我们想要查看一个很大的数据框中的某一列，我们可以使用美元符号和列名来选择每一栏。在这个示例中，我们可以直接输入美元符号和列名来输出这个列的内容。如果我们想要查看这个列中有多少个唯一的数值，我们可以使用unique函数。在这个例子中，我们可以看到这个列中有五个唯一的数值。 #使用$符号定位某一列 head(match.data.all$Label,10) ## [1] &quot;immoralSelf&quot; &quot;moralSelf&quot; &quot;moralOther&quot; &quot;immoralOther&quot; &quot;moralSelf&quot; ## [6] &quot;moralSelf&quot; &quot;moralSelf&quot; &quot;immoralOther&quot; &quot;moralSelf&quot; &quot;immoralOther&quot; #使用unique查看这一列有多少种独特的情况 unique(match.data.all$Label) ## [1] &quot;immoralSelf&quot; &quot;moralSelf&quot; &quot;moralOther&quot; &quot;immoralOther&quot; &quot;Label&quot; 5.3.1.1 filter() 因我们需要使用 filter 功能来筛选我们需要的数据。在本次实验中，我们采用了一个例子，以帮助大家更好地理解数据分析中的 filter 形式及使用方式。 最简单的方法是根据我们所拥有的变量来进行筛选，我们已知其中一个变量可以带来非常明显的效益，即匹配和不匹配的情况。在三个被试中，我们需要进行基本的查看，以确认其匹配和不匹配条件是否相同。为了进行这种分析，我们需要将原始数据分成两部分，即匹配和不匹配。 对于这个 data，我们可以使用 Dplyr 中的 filter 功能，以实现对数据的筛选。filter 的第一个参数是需要操作的数据集，而第二个参数是所需的筛选条件。对于本次实验来说，我们需要使用 filter 函数来提取匹配和不匹配条件相同的数据，即我们需要从 match 数据中提取所有 non-match 或 mismatch 数据。为了保证我们选择的数据正确，我们可以使用 unique 命令来进行检查，如果我们成功完成了筛选，则 unique 命令会返回一个值（即 mismatch）。 R中有一些用于比较的运算符，例如大于、小于、大于等于、小于等于。这些都是在做基础知识时非常重要的东西，我们可以在这里稍微细致地介绍一下。 在R语言中，我们还有一个特殊的运算符，就是!。我们在命名变量时，不能使用这个!，因为它表示的是不等于。例如a不等于b，实际上是先有一个等于符号，然后再用一个感叹号表示否定，这样它们就不相等了。我们这里使用的是两个等于号，表示完全相等的两个元素。 bijiao 有人会问，大于小于只能用于数字吗？实际上并不是这样。我们可以使用字符进行比较运算，但是要特别小心。因为有的时候你以为它在做数字运算，但它实际上是基于字符运算的。因此，我们需要在使用时格外注意。 在R语言中，还有一个非常有用的操作符，就是in。它是一个函数，用这个符号代表。它的作用是判断a是否是b中的一个元素。这也是我们在上一节课上讲到的概念，“向量”，它是一个一串的数字，我们需要找到其中的某个值。这时使用in操作符就可以帮助我们查找所有满足条件的值的位置。in操作符在R语言中非常受欢迎，因为它能够快速帮助我们找到一些特定的值。 在比较运算的时候，我们涉及到了“vector”。我们之前讲过，它就是一些数据的类型。但是对于“vector”而言，它有一个特点：里面的所有元素必须是同一个类型的。我们提到的有逻辑型、整型、双精度，字符型等类型。如果要把一些元素放在一起，那么它们应该都属于同一个类型。如果不是同一种类型，我们就可能需要使用其他类型的数据来存放它们。 回到刚才的代码，当我们使用“filter”操作数据时，我们是通过比较数据中的“match”这一列和“mismatch”这个字符串来得到的结果。因为我们需要把它与每一个对象进行比较，所以每个对象都会有一个输出。因此，我们得到了一个逻辑型的“vector”。我们可以把它们连成一个向量。在这个“match”中，我们可以使用“class”函数来查看某一列的数据类型。比如，我们查看一下刚刚的“match”是什么类型的，我们可以发现，它是字符类型的，它的输出结果是“correct”。对于其他类型，我们也可以做类似的查看。当然，字符类型除了“=”操作符外，还有其他方式可以用来确定它是否精确匹配。 #筛选出match.data.all的Match列中所有mismatch的数据 mismatch &lt;- dplyr::filter(match.data.all, match.data.all$Match == &quot;mismatch&quot;) #查看是否成功 unique(mismatch$Match) ## [1] &quot;mismatch&quot; class(match.data.all$Match) ## [1] &quot;character&quot; 新的DataFrame是根据条件筛选出的子集。在处理DataFrame时，我们可以按步骤来处理。如果没有现成的函数能够直接解决问题，我们可以通过自己的思考，将筛选过程拆成一个个小的步骤，逐步解决问题。首先，我们要提取出一个column，然后将column中的每个元素与条件进行比较。比较结果可以得到一个新的vector。接着，我们可以通过vector中的rowid提取出符合条件的ID，再从原始数据集中提取出对应的子集。如此，我们就完成了整个筛选的过程。再刚刚的过程中机器会自动将我们的需求划分成小的步骤来处理。 除了字符之外，我们也可以对数字进行筛选。首先需要将字符串类型的“acc”转换为数值型的变量。我们可以查看变量类型，如果是字符型，我们可以使用“as.numerical”的函数将其转换为数值型。通过match data.all这个命令，我们可以获取到一个字符型的vector”acc”，将其转换成数值型后，我们将其赋值给另一个名为”acc”的变量。这样，我们就将”acc”变为数值型了。通过查看unique命令，我们可以发现”acc”共有”-1,0,1,2”这几个值。我们可以将大于0的值作为筛选条件，这样筛选过的变量中只保留了0,1,2这几个值，从而完成了数字的筛选过程。 class(match.data.all$ACC)#发现类型是character而不是numeric，我们需要先转换。 ## [1] &quot;character&quot; #通过as.numeric()将其他类型转化为数值型向量 match.data.all$ACC &lt;- as.numeric(match.data.all$ACC) class(match.data.all$ACC) ## [1] &quot;numeric&quot; 在看这篇文章时，我们会发现我们需要筛选出0和1的结果，因为-1表示没有按键，2表示按错键。只有0表示错了而1表示做对了。因此，我们需要把结果筛选为0和1。有两种方法可以做到这一点。第一种方法是对结果进行两次筛选，首先做一个大于等于0的筛选，然后再做一个小于2的筛选。第二种方法是使用逻辑运算符adn和all，通过ACC大于等于0和ACC小于2的结果来筛选出0和1。这些逻辑运算符通常是将不同的逻辑连接在一起的常见做法。 #筛选出match.data.all的ACC列中所有大于等于0的数据 mismatch &lt;- dplyr::filter(match.data.all, match.data.all$ACC &gt;=0) #查看是否成功 unique(mismatch$ACC) ## [1] 0 1 2 在这个示例中，我们使用了ACC。首先将ACC与0进行比较，使其大于等于0。然后再将ACC与2进行比较，使其小于2。最后得到的两个结果可以使用逻辑运算符AND连接起来。如果前面的结果是True，同时后面的结果也是True，那么它们的AND结果就是True。如果其中一个结果为False，那么AND结果就是False。 或者，我们也可以使用逻辑运算符OR。使用OR时，当前面和后面的结果中至少有一个是True时，它们的OR结果就是True。如果前面和后面的结果都是False，那么它们的OR结果也是False。 总之，我们可以通过使用适当的逻辑符号来筛选结果，使其变得更清晰，更易读。 "],["lesson-5.html", "6 第五讲：如何清理数据—数据的预处理 6.1 批量读取文件 6.2 数据预处理 6.3 函数", " 6 第五讲：如何清理数据—数据的预处理 6.1 批量读取文件 在本节课中，我们将讲解如何对从问卷或实验软件中下载的数据进行预处理，如果我们想要进行回归或中介分析等，就需要对数据进行预处理。以得到我们最终想要分析的数据。我们将演示如何使用for loop或lapply函数将多个数据合并成一个完整的数据。 我们会给出一个完整的路径或相对路径，让R内部的计算机知道文件在哪里，并对其进行操作，比如read.csv。如果我们要读取一系列文件，我们需要将它们的相对路径读入，然后依次输入到处理器中，让它们被读取并合并成一个完整的数据框。因此，我们需要使用for循环或lapply函数将所有文件的路径列出来，并将它们输入到处理器中，让它们被读取并合并。 我们的输入是文件夹中的文件名。大家可以看一下自己电脑中的文件夹，会发现有很多子文件，比如practice、match和category，它表示的是三个时间的三个阶段 practice表示它在做练习，match表示它在做match的任务，category表示它在做categorization的一个任务。现在我们需要找出所有包含match且以out结尾的文件，因为这些是我们需要读取的文件。 6.1.1 通配符 我们不想逐个列出每个文件名，因为这样太冗长且容易出错。我们可以使用R中的list函数，将文件夹中的所有文件列出来，然后筛选只包含match的文件名。为了实现这一点，我们需要使用通配符，它是一种特殊的符号，可以匹配任意字符。 例如，*.csv代表以csv结尾的所有文件。我们可以使用list函数扫描文件夹，找到所有以match开头且以out结尾的文件名，然后将它们合并成我们需要的文件。我们要做的第一件事是扫描这个文件夹，扫描这个文件夹，把里面所有的文件和文件夹都读取出来。但是这样并不完全符合我们的目标，因为我们只需要符合match这个条件，并且是以out结尾的数据。所以，我们需要列出match文件夹里所有的文件，但这并不符合我们的目标。这时，我们需要使用通配符来匹配文件夹里包含特殊信息的文件。也就是说，我们需要根据文件名是否包含match来筛选文件夹里的文件。 这时，我们需要使用通配符，尤其是问号这个通配符。因为在计算机语言中，问号代表任意数量的任意字符。当我们以 星号.csv结尾时，代表我们只需要扫描以.csv结尾的文件，把它们全部读出来，读取它们的完整文件名。如果它不是以.csv结尾的，我们就跳过它。问号代表单个字符，也就是信号代表任意单个字符。例如，如果我们使用file?，然后是.txt，那么它能够匹配的符合条件的文件就是file1、file2、file3等等。但是如果你是file10，它后面有两个字符，这时它就不匹配了。 中括号里的字符是或的关系，就是说，中括号里的123代表file1后面跟1、2或3都可以。这样可以任意灵活地匹配。因为有可能你不知道文件夹里有多少个文件，你就把它们全部写在中括号里，只要它包含在里面，我们就把它读取出来。 library(DT) ## Warning: package &#39;DT&#39; was built under R version 4.2.3 # 所有路径使用相对路径 library(here) ## Warning: package &#39;here&#39; was built under R version 4.2.3 ## here() starts at C:/GitHub/R4PsyBook/bookdown_files/Books/Book # 包含了dplyr和%&gt;%等好用的包的集合 library(tidyverse) # 养成用相对路径的好习惯，便于其他人运行你的代码 WD &lt;- here::here() getwd() ## [1] &quot;C:/GitHub/R4PsyBook/bookdown_files/Books/Book&quot; 6.1.2 for循环思路 那么我们该怎么使用它呢？我们需要先加载tidyverse这个包，提醒大家在开始处理之前要load这个包，否则在使用函数时会出错。这是一个准备工作。接着，我们找到了当前的工作路径。 # 把所有符合某种标题的文件全部读取到一个list中 files &lt;- list.files(file.path(&quot;data/match&quot;), pattern = &quot;data_exp7_rep_match_.*\\\\.out$&quot;) head(files, n = 10L) ## [1] &quot;data_exp7_rep_match_7302.out&quot; &quot;data_exp7_rep_match_7303.out&quot; ## [3] &quot;data_exp7_rep_match_7304.out&quot; &quot;data_exp7_rep_match_7305.out&quot; ## [5] &quot;data_exp7_rep_match_7306.out&quot; &quot;data_exp7_rep_match_7307.out&quot; ## [7] &quot;data_exp7_rep_match_7308.out&quot; &quot;data_exp7_rep_match_7309.out&quot; ## [9] &quot;data_exp7_rep_match_7310.out&quot; &quot;data_exp7_rep_match_7311.out&quot; str(files) ## chr [1:44] &quot;data_exp7_rep_match_7302.out&quot; &quot;data_exp7_rep_match_7303.out&quot; ... 我们可以用两种方法来处理这个for循环，其中一种是使用一个变量名files来存储我们从这个文件夹里扫描到的所有文件的名字。在R中，list.files()是一个函数，它可以扫描文件夹里的所有文件。第一个参数是你要在哪个文件夹里扫描。即在当前的工作目录里的data文件夹中，然后在这个文件夹中的match文件夹里进行扫描。我们使用这个pattern来筛选文件夹里的文件名，比如包含了这个match的，或者更加精确的，我们要以这个data_exp7_，然后以repeat开头的文件。然后我们使用一个通配符来表示所有以out结尾的文件都扫描进来。这样扫描完后，我们就得到了所有符合我们这个模式的文件的名字。我们只会得到它们的文件名，不包含它们的路径。如果我们把它们读取出来并列出前面10个，我们就可以看到它们确实完全符合我们上面那个规则。list.files()函数非常有用，因为它可以帮助我们快速地扫描文件夹里的所有文件。 这个操作实际上是扫描一个文件夹，并按照特定模式扫描文件。在R语言中，我们经常使用这个操作来读取数据，这是第一步。我们现在读取到的是一个向量，其中每个元素包含许多字符，每个字符代表一个文件名。这与我们之前讨论的读取单个文件不同。现在我们正在列出一个文件夹中所有符合特定模式或规则的文件名。 读取完后，我们可以使用for循环。首先，我们需要创建一个空的列表来存储读取的数据。for循环的结构是for（条件）{内容}。我们使用i来循环，in表示我们要循环的范围。在for循环中，我们使用i来逐个循环。我们首先将i设置为1，然后做某些事情，然后在i等于2时再做同样的事情，一直到i等于10。 # 创建一个空的列表来存储读取的数据框 df_list &lt;- list() # 循环读取每个文件，处理数据并添加到列表中 for (i in seq_along(files)) { # 重复&quot;读取到的.out个数&quot;的次数 # 对每个.out，使用read.table df &lt;- read.table(file.path(&quot;data/match&quot;, files[i]), header = TRUE) #read.table似乎比read.csv更聪明，不需要指定分隔符 # 给读取到的每个.out文件的每个变量统一变量格式 df &lt;- dplyr::filter(df, Date != &quot;Date&quot;) %&gt;% # 因为有些.out文件中部还有变量名，所需需要用filter把这些行过滤掉 dplyr::mutate(Date = as.character(Date),Prac = as.character(Prac), Sub = as.numeric(Sub),Age = as.numeric(Age),Sex = as.character(Sex),Hand = as.character(Hand), Block = as.numeric(Block),Bin = as.numeric(Bin),Trial = as.numeric(Trial), Shape = as.character(Shape),Label = as.character(Shape),Match = as.character(Match), CorrResp = as.character(CorrResp),Resp = as.character(Resp), ACC = as.numeric(ACC),RT = as.numeric(RT)) # 将数据框添加到列表中 df_list[[i]] &lt;- df } # 合并所有数据框，只有当变量的属性一致时，才可以bind_rows # bind_rows 意味着把list中的所有表格整合成一个大表格 df.mt.out.fl &lt;- dplyr::bind_rows(df_list) # 清除中间变量 rm(df,df_list,files,i) # 如果你将这个步骤写成函数，则这些变量自然不会出现在全局变量中 在for循环中，我们使用df=read.table来读取文件。文件的路径是相对路径，是data match和files的组合。files是文件夹中的一个文件。当i等于1时，我们读取的是第一个文件。我们按照header为true的方式去读取。我们发现out文件里保存的可能不是那么统一，因此我们需要去掉重复的列名。我们可以用一个规则，即如果data再次出现，表示它重复了上面这个列名，我们就把所有这样的行去掉。我们用了上次课选择的那个筛选函数。 反向筛选掉之后，我们还可以对它的变量类型做一些变化。我们基本上就是用了as character和as numerical这两种，把所有的列都变成我们想要的类型。然后，我们生成了一个数据框，并进行了转换。当i等于1时，它就是对第一个文件做了这一番操作。然后，我们把它复制到数据列表里面，第一个位置就是第一个out文件，它经过了转换之后的数据框，就装到数据框列表里面的第一个位置。我们首先将第一个out文件转换成数据框df，并将其放入df列表的第一个位置。然后，我们通过循环读取id位中的第二个文件，并倒数10个，直到读取完所有10个文件。data列表的长度为10，其中包含我们读取的10个数据。 将所有数据放入列表后，我们可以使用bind_rows函数将它们合并成一个大的数据框。bind_rows是Dplyr中一个常用的函数，它可以通过行将不同的数据框合并。因此，data列表实际上是一个完整的列表，其中包含一个一个的out文件，经过初步转换后形成的数据框。由于它们的列都是一模一样的，我们可以直接将它们叠加在一起，最终得到一个完整的数据框。 这就是for循环的逻辑，我们首先读取我们想要的文件名，然后通过迭代的方式一个一个地读取它们，并将它们放入一个DataList中。然后，我们对这个List进行完整的合并。这个思路非常直接，大家都能够理解。当然，理解for循环的前提是我们要理解它的工作原理。为什么我每次讲for循环的时候，助教们可能觉得没有必要呢？因为for循环的使用实际上非常少。但是，我个人认为还是要讲一下for循环。因为for循环不仅可以用在这里，而且可以用在很多场景中。当你需要重复做某一件事情时，没有现成的函数可以帮你完成时，最简单的方法就是写一个for循环，它可以循环迭代帮你完成某一件事情。i可以替代很多东西，比如你有五个东西需要做同样的操作，你可以写一个i从1到5的for循环，将每个东西放进去做同样的操作。这样，你就不用一个一个复制你的代码，然后改变其中的一个值，再运行一遍。如果你经常进行批量处理，学会for循环会非常有帮助。为了更方便地读取并合并文件夹中的文件，我们通常需要使用for循环。 最终的结果应该是25920 obs 16 variables 6.1.3 lapply思路 但是，由于这是一个常见的操作，我们开发了一个更简洁的功能，即使用管道操作符“%&gt;%”来代替for循环。这个符号在DPLYR包中，它将前一步的结果作为下一个函数的输入。我们可以看到，每个符号代表一个操作步骤。我们首先列出符合条件的文件名，并将其作为一个列表输入到lapply函数中。lapply函数是一个apply系列的函数，它将函数应用于一个列表上。在这里，我们将read.table函数应用于列表中的每个元素，其中文件路径是x的参数，head=true是read.table的参数。这个操作实际上就是用一行命令代替了for循环的操作。 # 获取所有的.out文件名 df.mt.out.la &lt;- list.files(file.path(&quot;data/match&quot;), pattern = &quot;data_exp7_rep_match_.*\\\\.out$&quot;) %&gt;% # 对读取到的所有.out文件x都执行函数read.table lapply(.,function(x) read.table(file.path(&quot;data/match&quot;, x), header = TRUE)) %&gt;% # 对所有被read.table处理过的数据执行dplyr的清洗 lapply(function(df) dplyr::filter(df, Date != &quot;Date&quot;) %&gt;% # 因为有些.out文件中部还有变量名，所需需要用filter把这些行过滤掉 dplyr::mutate(Date = as.character(Date),Prac = as.character(Prac), Sub = as.numeric(Sub),Age = as.numeric(Age),Sex = as.character(Sex),Hand = as.character(Hand), Block = as.numeric(Block),Bin = as.numeric(Bin),Trial = as.numeric(Trial), Shape = as.character(Shape),Label = as.character(Shape),Match = as.character(Match), CorrResp = as.character(CorrResp),Resp = as.character(Resp), ACC = as.numeric(ACC),RT = as.numeric(RT) ) # 有些文件里读出来的数据格式不同，在这里统一所有out文件中的数据格式 ) %&gt;% bind_rows() 最终，我们得到一个包含所有数据框的列表，这些数据框都经过了一系列的转换和操作，然后使用“bind_rows”将它们合并成一个数据框。虽然这个过程看起来很冗长，但思路清晰。最终，我们得到一个包含16个变量和25000多个观测值的数据框。在读取数据后，我们通常会保存中间结果，以便下次使用。我们可以再次运行这个过程，但其实重新做一遍也无妨，因为数据基本上不会变。你只需要每次运行上面的代码，然后直接使用write.csv将读取的数据保存下来。write.csv很简单，只需要将csv文件写下来即可。我们前面整理并合并后的数据框需要一个名字和路径。我们可以使用相对路径，在当前工作目录下的data/match文件夹中将其保存为match-row.csv。一个常用的参数是强制将行名写入文件中，即row names等于false。 #for loop 或 lapply的都可以 write.csv(df.mt.out.fl, file = &quot;./data/match/match_raw.csv&quot;,row.names = FALSE) #write.csv(df.mt.out.la, file = &quot;./data/match/match_raw.csv&quot;,row.names = FALSE) 6.2 数据预处理 假设我们已经准备好了match和penguin的数据，我们可以读取数据并开始今天的数据预处理。我们使用刚才保存的csv文件，使用head等于true和separator等于逗号来处理数据。 在tidyverse中，filter和mutate是常用的功能之一。groupby可以根据某些变量对数据进行分组，但一定要记得使用ungroup。使用summarize可以计算均值、标准差、标准误等统计量。将groupby和summarize结合起来，我们可以快速有效地得到心理学中常用的统计量，如均值、标准差、标准误和计分数等。我们可以对数据进行分组和条件筛选。刚才提到的“ungroup”是指在“summarize”之后取消分组。而“select”函数是用来选择列，与“filter”函数选择行不同。有时我们也可以用“select”函数重新排序，而“arrange”函数则是用来对整个数据框按某一列的值进行排序。 接下来我们来看一个例子，假设我们要选择1995年或之后出生的人，我们可以在管道中使用“filter”函数。管道操作有两种方式，一种是直接从输入开始，另一种是用管道符号连接函数。在“dplyr”中，函数的参数是有顺序的，第一个是数据框。我们可以用点来代表输入数据，然后用“age”作为筛选条件。筛选完后，我们可以把结果输入到下一个函数中。如果要保留结果，我们需要把它复制到一个新的变量中。比如，我们可以选择与肃清障碍相关的问卷。 # 读取原始数据 df.pg.raw &lt;- read.csv(&#39;./data/penguin/penguin_rawdata.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) # 使用select选择age和ALEX的所有题目 df.clean.select &lt;- df.pg.raw %&gt;% dplyr::select(age, starts_with(&quot;ALEX&quot;), eatdrink, avoidance) #笨一点的方法，就是把16个ALEX都写出来 startwith是tidyverse中的一个包，可以方便地选择以“Alex”开头的所有列。它本质上是一个简化的通配符，因为在DPIY中，我们经常需要选择以某个特定开头或结尾的列，如果每次都写通配符会很麻烦。使用startwith包可以直接选择以“Alex”开头的列，并得到一串字符，可以和其他选择一起使用。 使用mutate函数可以生成一个新的变量，不仅仅是求和，还可以进行任意转换，比如加减乘除或判断。在这里，我们使用mutate函数将前四个“Alex”列的得分求和，得到一个新的变量“Alex3”，表示前四个“Alex”列的得分总和。 # 把ALEX1 - 4求和 df.clean.mutate_1 &lt;- df.pg.raw %&gt;% dplyr::mutate(ALEX_SUM = ALEX1 + ALEX2 + ALEX3 + ALEX4) 需要注意的是，它是逐行运算，可以使用其他函数如rowSums，其中也用到了通配符。我们可以利用DPRY中的筛选功能，选取所有以Alex为开头的列，并对这些列进行逐行求和，以得到真正反映Alex所有项目的总和。这种方法将给出16个项目的总和，而前面提到的方法只有4个。 # 对所有含有ALEX的列求和 df.clean.mutate_2 &lt;- df.pg.raw %&gt;% dplyr::mutate(ALEX_SUM = rowSums(select(., starts_with(&quot;ALEX&quot;)))) 此外，我们还可以使用mutate函数对数据进行重新编码，例如根据出生年龄将其分成不同的年龄段。我们可以使用case_when函数生成一个新变量，该变量根据条件变为不同的值。这种方法可以用于反向编码，例如将原来等于1的值变为5，将原来等于2的值变为4。这些技巧在心理学中非常常见，而case_when函数则是解决这些问题的好方法。 df.clean.mutate_3 &lt;- df.pg.raw %&gt;% dplyr::mutate(decade = case_when(age &lt;= 1969 ~ 60, age &gt;= 1970 &amp; age &lt;= 1979 ~ 70, age &gt;= 1980 &amp; age &lt;= 1989 ~ 80, age &gt;= 1990 &amp; age &lt;= 1999 ~ 90, TRUE ~ NA_real_) ) %&gt;% #当括号多的时候注意括号的位置 dplyr::select(.,decade, everything()) 我们可以先按照出生年代将数据进行分组，然后使用group by函数重新分组。我们还可以按照多个条件进行分组，例如按照年代和性别进行分组。分组完成后，我们可以使用summarize函数对每个组内部进行操作，例如求均值和标准差等。最后，我们可以使用ungroup函数将数据重新拆分，以便进行后续的运算。 df.clean.group_by &lt;- df.clean.mutate_3 %&gt;% dplyr::group_by(.,decade) %&gt;% # 根据被试的出生年代，将数据拆分 dplyr::summarise(mean_avoidance = mean(avoidance)) %&gt;% # 计算不同年代下被试的平均avoidance dplyr::ungroup() 我们可以将所有学到的函数串起来，例如1.先使用filter函数选择eat drink为1的base，然后2.使用select函数选择所需变量，3.再使用mutate函数对出生年份进行编码，4.最后使用groupby和summarize函数求出按照年代求alex的均值。5.最后，我们可以使用row sum函数对alex求均值。 df.pg.clean &lt;- df.pg.raw %&gt;% dplyr::filter(eatdrink == 1) %&gt;% # 选择eatdrink为1的被试 dplyr::select(age, starts_with(&quot;ALEX&quot;), eatdrink, avoidance) %&gt;% dplyr::mutate(ALEX1 = case_when(ALEX1 == &#39;1&#39; ~ &#39;5&#39;, # 反向计分 ALEX1 == &#39;2&#39; ~ &#39;4&#39;, ALEX1 == &#39;3&#39; ~ &#39;3&#39;, ALEX1 == &#39;4&#39; ~ &#39;2&#39;, ALEX1 == &#39;5&#39; ~ &#39;1&#39;, TRUE ~ as.character(ALEX1))) %&gt;% dplyr::mutate(ALEX1 = as.numeric(ALEX1)) %&gt;% dplyr::mutate(ALEX_SUM = rowSums(select(., starts_with(&quot;ALEX&quot;))), # 把所有ALEX的题目分数求和 decade = case_when(age &lt;= 1969 ~ 60, # 把出生年份转换为年代 age &gt;= 1970 &amp; age &lt;= 1979 ~ 70, age &gt;= 1980 &amp; age &lt;= 1989 ~ 80, age &gt;= 1990 &amp; age &lt;= 1999 ~ 90, TRUE ~ NA_real_)) %&gt;% dplyr::group_by(decade) %&gt;% # 按照年代将数据拆分 dplyr::summarise(mean_ALEX = mean(ALEX_SUM)) %&gt;% # 计算每个年代的被试的平均的ALEX_SUM dplyr::ungroup() # 解除对数据的拆分 在这里，我们可以按照多个条件进行分组，但我们只使用了一个条件，即数十年，因此可以写得很简洁。我们使用summarize函数计算alex总分的平均值，然后取消分组。如果我们对alex这个变量感兴趣，我们可以得到每个数十年在alex得分上的平均值。 在这个过程中，我们需要理解管道的操作方式，因为它可以处理我们想要对某个变量的所有领域进行的所有操作。每个函数都可以单独使用，但将它们放入管道中可以省略一些步骤。Tidyverse中有许多包，例如startwith、case1和rowsum，它们都是Dplyr和Tidyverse系统中的函数。另一个常用的包是tidyr，它可以使我们的数据更加整洁。在Tidyverse中，我们可以使用一些常用的函数，比如separate、extract、unite和pivot。其中，separate可以按照特定规则将一个变量分割成为几列，比如将日期按照年月日分成三列。extract可以提取一个或多个特定的字符串。unite则是将多个列合并成为一列。pivot则可以将宽格式数据转换为长格式数据或者将长格式数据转换为宽格式数据。 长数据和宽数据 那么，什么是长格式数据和宽格式数据呢？在SPSS中，我们通常使用长格式数据来记录实验数据，比如eprime数据。长格式数据的形式可能是这样的：每个实验对象有一个ID，然后有不同的条件和变量，比如年龄和性别。每个实验对象可能有多个条件，因此我们需要以行来记录数据。每一行代表一个观察值。例如，如果我们有一个问卷调查，其中有五个问题，那么我们将每个问题的回答记录在一列中，每一行代表一个受访者。相反，宽格式数据是指我们将变量记录在多列中，每一行代表一个观察值。例如，如果我们有一个实验，其中有五个条件，那么我们将每个条件的结果记录在一列中，每一行代表一个受试者。 长数据和宽数据是数据分析和处理中常用的两种不同的数据结构。 长数据通常表示为一列数据包含多个变量，每个变量在不同的行中重复出现。例如，一列包含所有参与者的年龄、性别和教育水平，每个参与者在数据集中有多行记录。长数据集通常适用于需要进行聚合和统计分析的情况。 宽数据则通常表示为多列数据包含多个变量，每个变量在同一行中出现。例如，一列包含参与者的年龄，另一列包含性别，第三列包含教育水平。每个参与者只有一行记录。宽数据集通常适用于需要进行分组和比较分析的情况。 两种数据结构都有其优缺点，具体使用哪种结构取决于数据的分析目的和方法。 当一个数据集中的每一行都是单个观察单位时，通常使用宽数据格式。下面是一些宽数据格式的例子： 一份包含人口统计数据的电子表格，每一行表示一个城市或地区，每一列则包含不同的人口统计数据，如总人口数、平均年龄、平均收入等等。 一个电商网站的订单数据集，每一行表示一个订单，每一列包含订单的属性，如订单号、购买日期、购买者姓名、商品名称、数量、价格等等。 一个医疗研究的数据集，每一行表示一个受试者，每一列包含该受试者的不同测量结果，如身高、体重、血压、血糖等等。 当数据集中的每一行表示的是一个观察单位的不同取值时，通常使用长数据格式。下面是一些长数据格式的例子： 一份学生考试成绩的数据集，每一行表示一个学生的一门考试成绩，每一列包含学生的属性（如学生ID、姓名、年级、学科等）和考试成绩。 一个心理学实验的数据集，每一行表示一个受试者在实验中的一次操作，每一列包含操作的属性（如受试者ID、实验条件、操作类型等）和操作结果。 一份股票市场的数据集，每一行表示某支股票在某个时间点的市场数据，每一列包含市场数据的属性（如股票代码、日期、开盘价、收盘价、成交量等）。 最后，我们需要注意数据中的缺失值。有时候，我们需要将缺失值删除，以便我们可以更好地分析数据。我们可以使用dropNA函数来删除缺失值，但是我们需要谨慎使用它，因为它可能会导致一些问题。 6.3 函数 6.3.1 函数参数 首先，我们看到的是一个大家已经很熟悉的函数，即read.csv。我们将这个数据读取进来，成为R里面能够操纵的一个变量。read.csv实际上是一个函数名，括号里面的就是我们要输入的参数argument。第一个argument是一个文件的路径，第二个是header，表示是否使用第一行作为我们的column names，第三个是sep，表示我们指定的分格符是什么。我们还有第四个，即force，表示是否把我们读到的文件里面的字符串作为factor。每一个函数基本上都包含两个部分，即函数名和argument。 function1(argument1 = 123,argument2 = &quot;helloworld&quot;,argument3 = list1) #第一种输入方法：同时输入argument和value function2(123,&quot;helloworld&quot;,list1) function2(&quot;helloworld&quot;,list1) #第一种输入方法：也可以省略argument,顺序读取value function2(&quot;helloworld&quot;,123,list1) 我们有几种输入argument的方法，第一种是完整的输入，即function name和argument都要输入。第二个方法就是 大家可以看到我们其实有的时候是直接写这个function name，就是value1, value2, value3对吧 我们没有加argument等于什么 就相当于我们没有把这个argument写出来 那么这个时候的话 我们还是把这个顺序调换一下 我们写上面一样 就直接是value3, value1, value2 最后大家想想这两个结果会反复是一样的结果吗 不一样对吧 为什么不一样呢 所以有同学已经知道这个了对吧 实际上是我们一种方式 我们完整的输出的时候 就是会把每一个value 比方说value1就复制到argument1 那么第二种方式的话 我们是按照顺序来输入的 那么按照顺序的时候 我们写函数的时候 每一个argument就是一二三这种排列下来的，一个是result1，它有三个参数，分别对应value1、value2和value3。第二个代码是把参数的顺序调换了一下，但是结果并不会改变。这是因为我们只是改变了参数的顺序，但是它们的值并没有改变。另外，如果我们只给出了值，而没有明确指定参数，那么函数会按照顺序把值赋给参数。如果我们只给出了一个值，那么函数会默认把它赋给第一个参数，而其他参数则会有默认值。我们可以在R中使用read.csv()函数。首先，我们可以查看帮助文档，使用问号加函数名的方式。对于read.csv()函数，除了第一个argument（文件名）以外，其余argument都有默认值。例如，header默认为false，separate默认为空格，quote为一个特殊符号表示引号。另外，string as fact也是一个默认值。因此，我们只需要输入文件名这一个argument即可。如果不输入其他argument，它会使用默认值。这是R的一个重要特点，它兼具灵活性和便捷性。如果我们要完整写出来，我们需要写出文件名这个argument的名字，即file。我们可以输入相对路径或绝对路径。其他argument的默认值可以在电脑上查看。因此，我们可以更简洁地写出这个函数，只需要输入三个argument即可。函数有两个部分，一个是函数名，一个是输入的argument。我们需要按照函数定义者的要求来使用这些argument。的函数都进行了修改，这就会导致我们之前写的代码出现问题。因此，在使用别人的函数时，我们需要时刻关注它的更新和变化，以便及时调整我们的代码。另外，我们也可以通过自己编写函数来满足特定的需求，这样可以更好地掌控代码的逻辑和功能。 6.3.2 groupby() groupby有两个作用：第一个是分组，然后将其拆分为不同的小包，然后在对分组后的数据进行分析时，它将以组为单位进行操作，将每个组的结果分别计算，然后组合起来。如果我们在没有任何操作的情况下添加groupby，它会如何改变我们的数据框呢？ # 读取原始数据 df.mt.raw &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) library(&quot;tidyverse&quot;) group &lt;- df.mt.raw %&gt;% group_by(Shape) group#注意看数据框的第二行，有Groups: Shape [4]的信息 ## # A tibble: 25,920 × 16 ## # Groups: Shape [4] ## Date Prac Sub Age Sex Hand Block Bin Trial Shape Label Match ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 02-May-201… Exp 7302 22 fema… R 1 1 1 immo… immo… mism… ## 2 02-May-201… Exp 7302 22 fema… R 1 1 2 mora… mora… mism… ## 3 02-May-201… Exp 7302 22 fema… R 1 1 3 immo… immo… mism… ## 4 02-May-201… Exp 7302 22 fema… R 1 1 4 mora… mora… mism… ## 5 02-May-201… Exp 7302 22 fema… R 1 1 5 immo… immo… match ## 6 02-May-201… Exp 7302 22 fema… R 1 1 6 immo… immo… match ## 7 02-May-201… Exp 7302 22 fema… R 1 1 7 mora… mora… match ## 8 02-May-201… Exp 7302 22 fema… R 1 1 8 mora… mora… match ## 9 02-May-201… Exp 7302 22 fema… R 1 1 9 mora… mora… mism… ## 10 02-May-201… Exp 7302 22 fema… R 1 1 10 immo… immo… mism… ## # … with 25,910 more rows, and 4 more variables: CorrResp &lt;chr&gt;, Resp &lt;chr&gt;, ## # ACC &lt;int&gt;, RT &lt;dbl&gt; 举个例子，如果我们将df.mt.raw复制到名为group的变量类中，那么如果我们直接将df.mat.raw放入其中，我们可能会看到前面的行数和列数。但是，如果我们添加了groupby，它将具有名为groups的东西，然后是形状和上面的表单。这意味着我们的表单仍然是原来的数据框，但现在它有一个标记，表示它已经在内部进行了分组，分组标准是group，即形状。如果您想删除此形状变量，就必须ungroup。否则，这个分组标签将一直存在于数据框中。因此，我们建议在进行groupby之后一定要进行ungroup，否则分组标签将一直存在于数据框中。 实际上，我们不会像现在这样无聊地直接添加groupby，然后看它会发生什么。我们需要明确后面分析的逻辑是什么。我们可以通过groupby将数据框按照base的ID分成几个小的数据框，然后以每个亚组为单位进行计算。比如，我们可以用summarize求出每个subgroup的行数，然后返回到n里面去，得到一个描述性的结果。但是，我们需要注意，当我们得到新的结果后，一定要把它ungroup掉，否则会影响后面的分析。此外，我们需要注意，如果我们再进行一个groupby，而没有ungroup前面的结果，可能会覆盖掉前面的结果，这是需要注意的问题。 如果我们不使用groupby，我们以前的做法是先申请一个中间变量，然后将数据按条件分组，选出一个subset出来，然后对每个操作分别求均值和标准差，最后将它们合并起来。现在有了groupby和summarize，我们可以在管道中一次性完成所有操作，而不需要生成大量的中间变量。这样做的好处是逻辑更清晰，代码更简洁，而且不会占用过多的内存。这就是关于groupby的说明。我们刚才展示的是使用和不使用group by的区别。使用group by可以得到每个条件下的函数值，而不使用则是整个数据的函数值。 对于group_by()函数的作用，我们可以对比不使用它的效果。 library(&quot;tidyverse&quot;) df.mt.raw &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) group &lt;- df.mt.raw %&gt;% group_by(.,Shape) %&gt;% summarise(n()) DT::datatable(group) ungroup &lt;- df.mt.raw %&gt;% summarise(n()) DT::datatable(ungroup) 6.3.3 定义函数 因为我们已经讲了函数的逻辑，即函数名、参数和操作。我们可以最简单的方式就是建立一个函数。我们用function来定义函数，它本身也是一个函数，用于帮助我们定义一个函数。我们可以在括号()中输入任何我们想定义的参数。然后在大括号{}中定义函数的操作，例如求和和乘积。 fun1 &lt;- function(a = 1,b = 100){ sum &lt;- 0 for (i in a:b) { sum &lt;- sum + i } return(sum) } 在这个函数中，a和b都有默认值，a的默认值是1，b的默认值是100。如果我们不给它们赋值，它们就会按照默认值运行。我们可以给a和b赋值，然后运行函数，得到sum的值。这种方式非常有用，因为有时候现有的函数可能不够满足我们的需求，我们可以自己定义一个简单的函数来处理数据。我们可以定义一个函数来进行实验操作和数据分析，这样可以方便后续的优化和重复使用。 在R中，括号有不同的用法，中括号一般用于调用变量中的数据，根据变量的不同特点，可能需要输入不同的index值。索引某个变量中的元素通常使用中括号，小括号通常用作函数的参数或输入。在函数中，小括号表示输入参数的值；大括号一般表示为一个代码块。if else语句中，小括号表示输入判断条件。在for循环中，我们也需要使用小括号和大括号来输入循环条件和操作。 fun1 &lt;- function(a = 1,b = 100){#第一个{构成最外层代码块的起始 sum &lt;- 0 for (i in a:b) {#第二个{构成次外层代码块的起始 sum &lt;- sum + i }#与第二个{对应的}构成次外层代码块的结尾 return(sum) }#与第一个{对应的}构成最外层代码块的结尾 6.3.4 for loop for (variable in sequence) {statement} for loop是一种循环语句，它的语法结构是for (variable in sequence) {statement}，其中variable是一个代符，sequence是一个向量或列表，而statement则是要重复执行的某一个语句。在每一次循环中，variable都被赋予为sequence里面的一个元素，然后执行一次statement，再回到sequence里面的下一个元素，不断地循环。每一次循环，variable都会变成sequence里面下一个元素，因此它只是一直在变化。 sum &lt;- 0 #variable i sequence 1:100 for (i in 1:100) {#statement sum &lt;- sum + i } print(sum) ## [1] 5050 举个例子，如果我们要对1到100的数据进行求和，我们可以定义一个sum变量，然后用for loop来实现。在这个例子中，i是variable，1:100是sequence，1到100的所有整数是1:100里面的元素。每一次循环，sum都会加上i的值，最终得到1到100的和。在第一次循环时，sum的初始值为0，i的值为1，所以sum会更新为1。在第二次循环时，i的值为2，sum的值为1，所以sum会更新为3。以此类推，直到循环结束，sum的值就是1到100的和。 files &lt;- list.files(file.path(&quot;data/match&quot;), pattern = &quot;data_exp7_rep_match_.*\\\\.out$&quot;) df_list &lt;- list() for (i in seq_along(files)) { df &lt;- read.table(file.path(&quot;data/match&quot;, files[i]), header = TRUE) df_list[[i]] &lt;- df } 在批量读取文件的for循环中，我们读取了所有以data experiment 7开头，以out结尾的文件，使用list.files函数匹配符合模式的所有文件，得到一列文件名list，然后循环list。我们需要对1到num files进行操作，这个sequence是从1开始的。在R和MatLab中，我们也是从1开始的，但在Python中，我们是从0开始的。 在for循环中，我们使用i in seq_along(files)来循环操作，从i等于1开始，然后一直到最后一个元素。当i等于1时，我们使用read table函数，并从files中提取出第i个元素，这个元素实际上是一个字符，也就是代表某一个特定的文件。 file path函数将前面的文件夹和后面的文件名整合成一个完整的文件路径。因此，read table函数的第一个argument就是这个file path。当进行了读取操作之后。这个df实际上就是第一个out文件的数据。 在datalist中，i代表第几个元素，使用df.list[i]来index list。在第一次循环中，我们将df给到df.list的第一个元素。 因此，当i等于1时，我们开始第一次循环，首先有一个files代表需要操作的文件。多比方说什么什么.out，对吧？第一个元素是什么，第二个元素也是什么.out，一直到最后一个元素。当我们i=1的时候，我们把它的第一个元素拿出来，然后把它拿到redtable里面去。这个filepath本身又是一个函数，我们把这个第一个文件夹的名字和第一个out文件路径加起来，就是file对于filepath这个地方。然后再开启redtable的第二个argument，那就是header对于true。通过这个for loop，我们读取了所有的文件，把读取的结果复制到了df.list里面的对应元素。这个时候，df-list就更新了。以此类推直到所有的files里面的所有数据都读取完，那么这个时候我们的df-list就是一个包含了所有数据的一个list，每一个元素代表一个base的一个数据。 6.3.5 信号检测 接下来，我们回顾上一节课的练习题。我们使用数据预处理的方法，求出match数据中的deep prime，这是一个信号检测论的指标，也称为敏感性指标。 我们可以将数据分为signal和noise，其中signal是指hit，对应的是correct rejection；而如果刺激中有noise，我们认为它是有信号的，这就是FA。如果我们报告时认为刺激中有信号，但实际上是noise，这就是Miss。根据信号检测论，我们将刺激分为match和mismatch，其中match是信号，而mismatch或nonmatch是noise。我们的反应是指我们呈现的刺激。如果反应是match，那么这就是hit。对于match的trial，如果我们的反应是mismatch，那么这就是信号减速论的miss。正确率是0，因为它是错误的。对于mismatch的trial，如果我们的反应是mismatch，那么这就是CR，正确率是1。FA的正确率是0。我们需要计算d’，它等于zheat减去zFA，这是我们敏感性的指标。我们需要用R来计算每个条件下的d’，因此我们需要使用group by。最终，我们将得到一个比例，其中包括base的id、条件和d’。 我们在实际使用这些函数时，是带着目的去使用的。想要计算击中率虚报率，我们首先要知道什么情况是击中，也要让计算机知道什么情况是击中，一旦分类了，计算的时候也需要进行判断。但在分类判断之前，我们要先告诉计算机什么是正确情况什么是错误情况，哪些是这个被试做的，哪些不是，所以也需要分组。 我们有了基本的思路：1.分组，使用group_by。2.告诉计算机什么是hit，什么是miss，使用summarise函数。3.利用判断语句进行分类计算，我们这里使用了ifelse。 我们选出需要的几列，包括自变量和因变量，以及分组变量。使用select()函数，因为数据中存在缺失值，所以也需要除去它们。 df.mt.clean &lt;- df.mt.raw %&gt;% dplyr::select(Sub, Block, Bin, # block and bin Shape, Match, # 自变量 ACC, RT, # 反应结果 ) %&gt;% tidyr::drop_na() 在计算击中率误报率等的时候，我们针对的是被试的在某一特定实验条件下的反应。所以我们需要通过bin、block、shape、sub进行分组，这样进行计算时，就是每一个条件组下分别计算。不会出现在虚报的实验条件下计算正确拒绝的情况。 df.mt.clean &lt;- df.mt.raw %&gt;% dplyr::select(Sub, Block, Bin, # block and bin Shape, Match, # 自变量 ACC, RT, # 反应结果 ) %&gt;% tidyr::drop_na() %&gt;% #删除缺失值 dplyr::group_by(Sub, Block, Bin, Shape) 接下来就是要使用summarise函数来给击中、虚报等分类，并使用ifelse函数根据分类计算概率 df.mt.clean &lt;- df.mt.raw %&gt;% dplyr::select(Sub, Block, Bin, # block and bin Shape, Match, # 自变量 ACC, RT, # 反应结果 ) %&gt;% tidyr::drop_na() %&gt;% #删除缺失值 dplyr::group_by(Sub, Block, Bin, Shape) %&gt;% dplyr::summarise( hit = length(ACC[Match == &quot;match&quot; &amp; ACC == 1]), fa = length(ACC[Match == &quot;mismatch&quot; &amp; ACC == 0]), miss = length(ACC[Match == &quot;match&quot; &amp; ACC == 0]), cr = length(ACC[Match == &quot;mismatch&quot; &amp; ACC == 1]), Dprime = qnorm( ifelse(hit / (hit + miss) &lt; 1, hit / (hit + miss), 1 - 1 / (2 * (hit + miss)) ) ) - qnorm( ifelse(fa / (fa + cr) &gt; 0, fa / (fa + cr), 1 / (2 * (fa + cr)) ) )) ## `summarise()` has grouped output by &#39;Sub&#39;, &#39;Block&#39;, &#39;Bin&#39;. You can override ## using the `.groups` argument. 在这里，我们使用了一个num来判断它们的数量。我们可以使用nrow代替。这只是其中一种做法。接下来，我们看一下acc里面所有等于match的情况。acc等于什么呢？条件是match，然后acc等于1。当我们想要计算hit时，我们需要将两个条件进行组合。一个是刺激呈现的内容，另一个是正确率。只有在match条件下反应正确的试次才是hit的试次。通过这个length，我们计算出了在每一个base、每一个block、每个bin下面以及每一个条件shape之下有多少个hit的比值。 同样的逻辑，我们通过mismatch和0计算出了FA（false alarm）。如果match的accuracy是0，我们就错失了这个信息，也就是miss。这是我们前面知识的一个简单应用。 df.mt.clean &lt;- df.mt.raw %&gt;% dplyr::select(Sub, Block, Bin, # block and bin Shape, Match, # 自变量 ACC, RT, # 反应结果 ) %&gt;% tidyr::drop_na() %&gt;% #删除缺失值 dplyr::group_by(Sub, Block, Bin, Shape) %&gt;% dplyr::summarise( hit = length(ACC[Match == &quot;match&quot; &amp; ACC == 1]), fa = length(ACC[Match == &quot;mismatch&quot; &amp; ACC == 0]), miss = length(ACC[Match == &quot;match&quot; &amp; ACC == 0]), cr = length(ACC[Match == &quot;mismatch&quot; &amp; ACC == 1]), Dprime = qnorm( ifelse(hit / (hit + miss) &lt; 1, hit / (hit + miss), 1 - 1 / (2 * (hit + miss)) ) ) - qnorm( ifelse(fa / (fa + cr) &gt; 0, fa / (fa + cr), 1 / (2 * (fa + cr)) ) )) %&gt;% dplyr::ungroup() %&gt;% select(-&quot;hit&quot;,-&quot;fa&quot;,-&quot;miss&quot;,-&quot;cr&quot;) %&gt;% dplyr::group_by(Sub, Shape) %&gt;% tidyr::pivot_wider(names_from = Shape, values_from = Dprime) ## `summarise()` has grouped output by &#39;Sub&#39;, &#39;Block&#39;, &#39;Bin&#39;. You can override ## using the `.groups` argument. 然后，我们计算出了每一个d’。我们可以看到，这个d’是用qnorm计算的。hit的比值是等于hit除以qt加miss。这个地方有点复杂，它实际上就是想要把我们这两个东西打包在一个语句里面。zheat和zfa有两个条件，首先我们看它的两个部分，一个是qnorm，表示heat的部分，另一个是fa部分。我们计算出heat rate，然后将其转换成z分数，减去quantum。这个地方为什么要用if else呢？因为会出现一个情况，比方说我的heat全部是正确的，这个时候正确率为1。正确率为1的话，我们用这个qnorm的话，它是一个无限大的一个，就是一个正向的infinity。对这种情况，我们要进行一个转换，就是我们要把这个heat rate转换成一个小于1的值，这样的话就避免它成为了一个infinity。如果它变成infinity的话，我们后面就没法进行计算了。所以，如果heat rate小于1，我们就用它。 这个是if else在这个type里面。如果前面heat rate…Heat rate相比较而言存在差异，如果它小于1，那么我们就使用它自己的值。如果它不小于1，也就是等于1，那么我们就用1减去1除以括号里的值，这样可以使得反应变得稍微小一点。这是一个常用的方法，在信号减速论里面也常用。对于FA也是同样的方法，我们使用if else，如果它大于0，那么我们就用它，如果等于0，那么我们就需要想一个办法让它变成一个不是0的值，因为如果它是0的话，那么它会变成负向无穷大的值，这样就无法进行后续的运算。接下来我们就可以算出d’，然后去掉后面所有的heat、fn、least和cr，进行后续的运算。这个逻辑清晰吗？ 我们可以拆开每一个函数来看，选择columns group by，summarize group by，qvolume是一个把百分比转换成为1分数的函数，这个大家需要去后面查找。然后我们进行相减，插入条件语句，再把它转换成1分数，最后就得到d’。我们也可以进行后续的运算，再次group by，subject和shape。我们可以查看每一个被试在每一个bin上面的结果的变化。如果我们收了很多被试，可能就不需要关注每一个被试的变化，只需要报告总体的结果即可。我们可以根据time inversion里面的数据预数的一系列操作，一个一个管道全部下来，最后直接得到我们想要的结果。这样的管道操作可能需要不断迭代，刚开始可能只会做最简单的预处理，但是迭代到最后，就会形成一个很长的管道。 数据处理的流程非常清晰。在这个过程中，我们对acc进行了选择，相当于是选择了符合条件的一些行，并求出它的长度。虽然理论上讲，我们也可以用别的变量来代替acc，比如rt，但是acc是最方便的选择，因为我们后面也会用到它。但是，这并不是唯一的操作，我们还可以用其他的变量来进行操作，比如length换成n。如果我们用filter的话，也是可行的，但可能需要写更多的代码。我们要根据自己的研究目的或者想要操作的目的去进行数据处理，把各种预处理操作进行组合。 "],["lesson-6.html", "7 第六讲：如何探索数据: 描述性统计与数据可视化基础 7.1 描述性统计 7.2 ggplot2的基本使用 7.3 Data Explore", " 7 第六讲：如何探索数据: 描述性统计与数据可视化基础 7.1 描述性统计 本节课我们将讲探索性数据分析和数据可视化。探索性数据分析是数据科学中的一个重要概念，它可以帮助我们发现数据中的规律和问题。 在传统的心理学中，我们通常会清楚地知道要进行什么样的分析，但是在数据科学中，我们可能面临的是一个未知的数据集，我们不知道其中的规律和数据的结构。因此，探索性数据分析非常重要。 在本节课中，我们还将介绍数据可视化的方法，它可以帮助我们更好地理解数据。 在进行数据分析时，如果我们面对的数据结构不清晰，甚至不确定数据是否干净，那么探索性数据分析（EDA）就变得非常重要。EDA的目的是在我们不确定数据结构和分析方法的情况下，通过描述和可视化等方式初步了解数据，从而帮助我们决定下一步使用什么样的统计方法或数据分析方法。在现在的大数据时代，EDA已经成为数据科学家们非常普遍的操作。 进行EDA时，我们需要了解数据的基本信息，比如数据的列数、变量类型、变量值的范围以及变量之间的关系等。 # 检查是否已安装 pacman if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)) { install.packages(&quot;pacman&quot;) } # 如果未安装，则安装包 # 加载所需要的R包 pacman::p_load(&quot;tidyverse&quot;) # 读取数据 df.pg.raw &lt;- read.csv(&quot;./data/penguin/penguin_rawdata.csv&quot;, header = TRUE, sep=&quot;,&quot;, stringsAsFactors = FALSE) df.mt.raw &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) 我们首先读取数据，这是我们之前都已经很熟悉的两个读取数据的语句。读取数据了之后，我们的命名方式现在基本上已经固定下来。比方说这个human-penguin-project的这个数据的话，我们就叫做df.pg.raw，这是原始的数据。然后对于那个match的数据的话，我们就叫df.mt.raw。 colnames(df.mt.raw) # 查看列名，观察有哪些变量 ## [1] &quot;Date&quot; &quot;Prac&quot; &quot;Sub&quot; &quot;Age&quot; &quot;Sex&quot; &quot;Hand&quot; ## [7] &quot;Block&quot; &quot;Bin&quot; &quot;Trial&quot; &quot;Shape&quot; &quot;Label&quot; &quot;Match&quot; ## [13] &quot;CorrResp&quot; &quot;Resp&quot; &quot;ACC&quot; &quot;RT&quot; DT::datatable(head(df.mt.raw, 3)) # 了解数据内容 我们可以查看每一个数据框里面到底有哪些变量名，用这个colnames。对于我们这个column names比较有限的情况之下的话，我们其实可以用这种方法。比方说像这个mt的这个数据，我们column names之后的话就可以查看它所有的columns的这个名字。假如说它有几百行几千列的话，这个时候就这个命令就不适用了。 另外的话，我们就可以通常用这个head，就是这个用这个head命令来查看它有前面三行长什么样子。这是最开始了解这个数据。 我们还可以使用一个str命令。我们也可以用summary，比方说我们可以简单的找到它的这个数据的一个概况，就用summary这个函数。summary函数输出之后它也是一个data frame，然后我们也可以去这么查看。 如果我们想要去了解这个变量里面的一些我们常用来描述数值的一些统计量，比方说像平均值中位数标准差等等这种，也就是我们说的集中量数和差异量数。那么我们可以用就是psych这个包。那么psych这个包它实际上是主要用来做问卷分析的一个包。大家如果做问卷分析比较多的话，可能会对这个包会越来越熟悉。那么这个包里面有个describe这个函数，是用来描述这个数据的情况的。类似的还有像那个Bruce R那个包里面，它也有一个Describe，它也是可以帮助我们描述一个数据里的情况。 DT::datatable(summary(df.mt.raw)) #或者使用psych包 DT::datatable(psych::describe(df.mt.raw)) 如果我想知道变量的平均数、中位数和标准差等统计量应该怎么办？在我们的数据框中，每个变量（即列）都有一个 n 表示行数，均值、标准差、中位数等常用的描述数据的值也会显示出来。如果我们想提取这些值，可以使用 summarize() 函数。例如，我们可以使用管道将数据框作为输入，然后使用 summarize() 函数来计算某个特定变量（例如 RT）的均值、标准差和行数。我们也可以自己找到感兴趣的变量的值。 # 使用dplyr包中的summarise()函数 df.mt.raw %&gt;% summarise(mean_RT = mean(RT), sd_RT = sd(RT), n_values = n()) ## mean_RT sd_RT n_values ## 1 0.7149504 0.1508394 25920 # summarise函数不会忽略缺失值，如果计算的列中有缺失值，会有报错。 需要注意的是，许多汇总数据的函数（例如均值或方差）不会自动忽略缺失值，因此我们需要在计算均值时添加一个参数（例如 na.rm = TRUE）来忽略缺失值。在使用 mean() 函数时，我们可以使用 ?mean 命令来查看具体的参数输入。 总之，我们可以使用描述性统计指标（例如均值和标准差）来了解数据框的情况。 7.2 ggplot2的基本使用 7.2.1 了解ggplot2 另一种方式是通过可视化的方式来了解数据。我们可以使用 ggplot2 包中的图层来构建图形，这是一种基于语法的图形描述方法。所谓gg源于“grammar of graphics”，即图形语法。 GGplot2是一种图形语法，它的核心是用图层来描述和构建图形。我们可以将数据映射到不同的图层中，然后将这些图层叠加起来形成最终的图形。 比如，我们可以用GGplot2来展示被试的正确率情况。我们可以从这个包中调用一个叫做GGplot的函数，然后将数据框作为参数传入。 aes(Aesthetics)是一个映射关系，它决定了如何将数据映射到图形空间中，并选择使用什么样的几何图形进行可视化。在GGplot2中，我们可以选择使用不同的几何图形，比如bar。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=ACC)) + # 确定映射到x轴的变量 geom_bar() + # 绘制直方图 theme_classic() # 设定绘图风格 此外，我们还可以选择不同的绘图风格，比如classical。GGplot2中有三个非常重要的成分，大家需要在自己的电脑上完成这个练习，否则很容易忘记。 我们可以使用数据来练习GGplot2的使用。首先，我们需要将数据读入到df.mt.row中，然后使用GGplot2来绘制图形。我们需要先加载tidyverse库，然后读取数据，最后才能开始绘图。 由于前面的课程已经讲解了如何使用library和读取数据，因此这节课是基于前面的数据处理知识的。我们需要回到前面的数据处理知识点，了解如何读取和合并数据。如果你没有保存数据到csv文件中，那么你可能会遇到找不到文件的问题。这个知识点是高度依赖的，前面的知识是后面的基础。如果你在使用前面的代码合并数据时速度太慢，你也可以使用U盘复制一下match_row.csv。 我们可以使用一个大的data frame，其中有很多行和列，其中一个是acc，它有很多值，例如1、0、0、1、1。 首先，我们需要使用一个命令来指定数据集，即以data frame作为输入，然后使用data参数和aes参数将数据映射到图层上。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=ACC)) + # 确定映射到x轴的变量 geom_bar() + # 绘制直方图 scale_y_continuous(expand=c(0, 0)) + # x轴在 y=0 处 theme_classic() # 设定绘图风格 现在，我们将x设置为ACC，这样我们只选择以它作为x轴。此时，它自动将x轴合并，并找到了它的独特取值，即-1、0和1。我们可以看到，如果将数据映射到二维空间中，它只有x轴，没有y轴。它自动帮我们补全了y轴，使用的是count，即每个取值的计数。 我们将这个数值映射到一个几何图形中，例如线段、点或条形图。我们选择使用条形图来表示count，因为它适合描述计数数据。因此，我们将数据以直方图的方式进行表示。最后，我们指定了以classical主题呈现图形，ggplot抱包含了很多预设的主题，可以根据自己的需要进行选择。在这个代码中，我们可以注释掉theme_classical()，然后运行代码，看看结果是否正确。如果没有出错，我们会发现画出的图形与之前不同，它有一个灰色的背景和方格。实际上，如果我们不使用theme_classical()，R会使用默认的风格来画图，这个风格可能与我们想要的不同。因此，使用theme classical可以帮助我们选择适合我们要的风格来进行可视化。 此外，我们还注意到，R的默认风格与我们在心理学或社会科学中常用的图形有所不同，例如字比较小，X轴的0点位置也不同。如果我们要在论文中呈现出这个图形，我们需要改变这些默认设置。我们可以使用scale_y_continuous命令来调整Y轴的连续数据，例如将X轴固定在0的位置。我们可以使用“scale y-continuous”命令来调整Y轴的连续数据。如果Y轴是离散数据，我们需要使用“discrete”命令。此外，我们可以在“theme”中调整字体、颜色和大小等细节，以满足特定目的。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=RT)) + # 确定映射到x轴的变量 geom_histogram() + # 绘制直方图 stat_bin(bins = 40) + # 设定连续变量分组数量 scale_x_continuous(name = &quot;RT&quot;) + # 命名x轴 scale_y_continuous(expand=c(0, 0)) + # x轴在 y=0 处 theme_classic() # 设定绘图风格 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 7.2.2 密度图 对于离散数据，我们可以使用条形图来表示，而对于连续数据，我们可以使用直方图或密度图。在直方图中，我们使用高度来表示数据的多少，而在密度图中，我们使用平滑曲线来表示数据的分布情况。在绘制图形时，我们可以使用“geom”命令来选择几何图形，如条形图或密度图。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=RT)) + # 确定映射到x轴的变量 geom_density() + # 绘制密度曲线 scale_x_discrete(name=&quot;RT&quot;) + # 命名x轴 scale_y_continuous(expand=c(0, 0)) + # x轴在 y=0 处 theme_classic() # 设定绘图风格 7.2.3 叠加绘图 我们可以将直方图和密度曲线叠加在一起，这样更能看到它们的分布情况。在这里，我们常用的叠加方式是将alpha参数设置为透明度，数值从0到1之间变化，越小表示透明度越高。我们可以将aes放在geom里面，这样就能够将图形相互叠加，看得更加清楚。alpha是多个条件画图时非常常用的参数。建立好数据映射关系后，我们可以先画一个直方图，然后在其上叠加一个密度图。 如果我们不想显示alpha设置透明度对应的图例，可以用guide隐藏起来。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=RT, # x轴的变量 y=after_stat(density), # y轴对应的是密度曲线 alpha=0.8)) + # 透明度 geom_histogram() + # 绘制直方图 geom_density() + # 绘制密度曲线 guides(alpha=FALSE) + # 隐藏透明度alpha设置带来的图例 theme_classic() # 设定绘图风格 ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use &quot;none&quot; instead as ## of ggplot2 3.3.4. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 箱线图也是我们常用的一种图形，其中黑线表示median，box表示50%的quater，25%和75%的quater。如果我们将x换成一个以上的变量，就会出现四个以上变量的取值。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=Label, # 确定映射到xy轴的变量 y=RT)) + geom_boxplot() + # 绘制箱线图 theme_classic() # 设定绘图风格 我们以四个独特的取值作为x，然后按照这四个条件将Rt分成四组。每一组都画成一个box plot，按照x的顺序排列，形成四个box plot的图。因为Rt是连续的数据，我们可以对它进行计算统计指标，例如median和四分位。这些数据分成上下两部分，上面50%下面50%、下面25%下面75%、上面25%下面75%。我们可以看到大量的数据都集中在这个附近，同时也可以看出它有一些变化。这主要是让我们初步了解不同条件下的反应情况。 另一个值得观察的是label。它按照字母顺序排序，因为它是字符类型的数据。在画图时，它会按照abcd的顺序进行排序。所以我们可以看到这个immoral在other之前，而i在m之前。这个顺序不一定是我们想要的，所以有时我们需要改变它的顺序。我们可以将Lable改为factor，并确定它的levels。然后我们可以使用mutate函数，例如label=factor(levels=c(“c”,“b”,“a”,“d”))，来改变它的顺序。这是画图时需要注意的小细节。 7.2.4 散点图 如果我们有两个变量，想要描述它们之间的关系，我们需要进行探索性分析。我们需要把x轴和y轴分别赋值，然后用点来描绘它们之间的关系。比如，我们可以看一个人在前测和后测的时候，他的体温的变化。我们可以用ggPlot来描述这个关系，x轴是temperature T1，y轴是temperature T2。 ggplot2::ggplot(data=df.pg.raw, # 指定数据 aes(x=Temperature_t1, # 确定映射到xy轴的变量 y=Temperature_t2)) + geom_point() + # 绘制散点图 scale_x_continuous(name = &quot;Temperature_t1&quot;) + # 修改X轴的名称 scale_y_continuous(name = &quot;Temperature_t2&quot;) + # 修改Y轴的名称 theme_classic() # 设定绘图风格 ## Warning: Removed 30 rows containing missing values (`geom_point()`). 我们可以用点来描绘这个数据的关系，如果它们沿着对角线，就是一个高度相关的关系。我们也可以加一个回归线，用LM来拟合一个平滑的线来代表它们之间的关系。 当然，在进行数据探索之前，我们也可以对数据进行预处理。比如，我们可以用penguin data来求出两个问卷的平均分。如果我们需要求出两个问卷的平均分。我们可以使用penguin raw data作为输入，然后使用mutate命令生成两列新的变量。一列叫做stress_ave，表示stress的平均得分。另一列是对手机的依赖，我们同样可以求它的均分。这样我们就得到了两个新的变量，一个叫做stress_ave，另一个叫做phone_ave。 # 利用管道符，可以帮助我们更简洁地合并数据处理和可视化的过程。 df.pg.raw %&gt;% dplyr::mutate(stress_ave=rowMeans(.[,c(&quot;stress1&quot;, &quot;stress2&quot;, &quot;stress3&quot;,&quot;stress4&quot;, &quot;stress5&quot;, &quot;stress6&quot;,&quot;stress7&quot;, &quot;stress8&quot;, &quot;stress9&quot;,&quot;stress10&quot;, &quot;stress11&quot;, &quot;stress12&quot;,&quot;stress13&quot;, &quot;stress14&quot;)]), phone_ave=rowMeans(.[,c(&quot;phone1&quot;,&quot;phone2&quot;,&quot;phone3&quot;,&quot;phone4&quot;,&quot;phone5&quot;, &quot;phone6&quot;,&quot;phone7&quot;,&quot;phone8&quot;,&quot;phone9&quot;)]) ) %&gt;% ggplot(aes(x=stress_ave, y=phone_ave)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + # 在散点图上叠加回归线，语法可以查找帮助文档 theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 18 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 18 rows containing missing values (`geom_point()`). 然后我们把这个data frame作为输入到ggplot里面去，X轴是stress的均值，Y轴是对手机依赖的均值。我们用这个几何图形来表达我们的数据，然后用smooth找到它们相互之间关系的一个回归线。这个回归线是用的linear model，它还有一个回归线的CI，就是95%的一个置信区间。最后是我们这个画图的主题。看到阴影，实际上是R里默认的一个输入，它不仅有一个回归线，还有一个回归线的95%置信区间。 ggplot2是数据可视化中非常重要的工具，它可以帮助我们化繁为简，不需要为每个参数设定值，因为它有很多默认选项。这样，我们可以快速地进行数据探索，而不需要关注太多细节。 虽然图表中有许多元素和参数，但我们可以手动控制每个部分，如点的大小、颜色、XY轴的名字、刻度和字体大小等。我们可以在同一图表中叠加多个元素，如散点图、回归线、轴和分布等，以显示丰富的信息。ggplot是一个非常丰富的生态系统，包含许多不同类型的图表，我们可以根据需要进行选择。 通过Tidyverse下面的数据处理，我们可以直接用ggplot画图，从原始数据到最终图表都可以一个管道完成。ggplot是一个非常常用的工具，可以精确地定制我们想要呈现的图表，并直接输出为PDF格式，方便提交给杂志。 7.3 Data Explore 此外，Data Explore也r是一个很不错的数据探索工具，可以帮助我们快速探索数据。我们可以使用安装工具包来实现可视化，比如plot_string，它可以将DataFrame中的所有列名以可视化的形式表达出来，类似于思维导图中的树形图。我们可以试一试。另一个是plot_intro，它可以显示一些信息，比如有多少个离散数据列，有多少个连续数据列等等。我们可以看到，对于我们的匹配数据，离散列占56％，连续列占43％，所有列都是缺失值的占0％。每个数据至少都有一些值，完整的行占97.46％。缺失观测值的数量也可以通过可视化方式快速了解。 # load DataExplorer pacman::p_load(&quot;DataExplorer&quot;) DataExplorer::plot_str(df.pg.raw) DataExplorer::plot_intro(df.mt.raw) 这是数据探索包的一个独特特点，它可以帮助我们快速可视化数据。关于缺失值，我们可以使用plot_missing命令将具有缺失值的列可视化。大多数列都没有缺失值，只有一个响应列有2.5％的缺失值。 DataExplorer::plot_missing(df.mt.raw) 我们可以看到，几乎所有数字化变量的计数都可以用条形图表示。例如，性别可以用female，male，2和1表示。我们可以看到，大多数人是右撇子，而匹配和不匹配的比例是一致的。如果我们在匹配条件下看到匹配比不匹配或不匹配更多，那么可能存在问题，因为我们的实验设计是一致的。同样，我们的实验条件应该是平衡的，因此看起来应该是一模一样的。 DataExplorer::plot_bar(df.mt.raw) 我们可以使用plot bar将所有变量以bar图的形式呈现出来。我们还可以根据match条件将数据分成matched和mismatched两部分，并用bar图表示每个部分的比例。在大多数情况下，matched和mismatched是平衡的。我们还可以使用histogram来快速绘制所有变量的分布情况，特别是连续变量的分布情况。我们可以使用qq plot来检验数据是否符合正态分布。 DataExplorer::plot_correlation(na.omit(df.pg.raw[, 2:30])) 我们也可以绘制一个表之间的相关矩阵，这样就更有意义了。这个函数na.omit对我们来说非常有意义，因为它可以去除缺乏值的行，否则会报错。 我们前面介绍了一个非常常用的工具ggPlot，它是我们整个可视化中非常常用的一个工具。我们还介绍了一个用于数据探索的工具包Data Explorer，它集成了大量的函数，可以快速探索数据结构。大家可以去官网上看看，Data Explorer还有其他一些功能。现在大家可以根据我们课程中的代码来进行修改，练习自己写代码。 大家可以在Penguin Data里面找出两个自己感兴趣的变量，然后画一个散点图。要选择另外两个变量的话，首先你要知道有哪些变量，所以大家可能先要去查看一下这个数据包含哪些变量，然后选择两个变量画一个散点图来做练习。 今天我们布置一个小作业，大家可以在match的数据里面选出一个被试，然后把它每个条件以shape作为条件的话，就是画出它的那个反应式的分布。大家可以任意选出一个被试，画八个图出来，画它们的反应式的分布，然后用match和shape分开画，画这个直方图加上这个density plot。最基本的要求就是画出八个图，比方说shape就是immoral self match，match对吧，然后或者是这是第1个图immoral self mismatch第2个图，然后就是比方说immoral other match immoral other mismatch，这样排列组合你最后得到八个图。 你可以一个一个地画出这些图，这是最简单的作业。然后你可以发挥你的探索能力，对它进行更加 fancy 的呈现，比如进行排列组合，最后把它放到一个图里面。 我们刚刚讲到了 alpha 值，你可以画一个有两个条级条的图，然后用 alpha 值对它进行叠加，这样你就能把两个图放在一起，但仍然很清晰。当然，还有其他方式，比如你可以分开画这八个图，也可以把它们拼起来放到一个大图里面。这是一个有相当大自由度的小作业，基本的要求是把这八个图画出来，从读取数据开始到数据的选择，最后到画图。 最后我们希望大家都能够理解这个结构，我们的文件夹里面有这些数据，我们要在这个根目录里面进行操作。如果你的根目录跟我们不一样，我们就需要改一下代码，把它拼出来。每个同学都要选不同的被试，然后画出八种条件下的八个图，怎么画，怎么拼图，这个时候大家可以有自由发挥的空间。 在使用 R 的时候，我们肯定会不断报错。看到报错之后，第一个不应该是慌了，你应该仔细看这个报错到底说的是什么东西。这个报错本身提示了很多信息，它不是没有意义的。实际上你应该仔细阅读一下，它这个错误的信息到底提示你哪个地方是错了。所有的基本上90%的解决方案都是可以在这里面找到的。 读取match数据，对自己感兴趣的变量进行描述性统计。 读取match数据，对不同shape的击中率进行分组绘图，可使用boxplot观察差异。 读取penguin数据，选择自己感兴趣的两个变量进行处理并画出散点图。 对两个数据中自己感兴趣的变量们做探索性数据分析。 第二个练习是读取 match 的数据，根据不同的限制的击中率进行分组画图。现在我们需要回到上一节课，将deep prime的击中率数据保存下来，并根据这些数据进行画图。这是一个很综合的应用。 OK，我再重复一遍，第一次小作业，我们的数据是match的数据，只有我们这个matching task的数据。大家可以随便挑出一个base来，然后对这个base的数据，画它的反应时分布。反应时的分布，大家用什么方法，用什么图都可以。但是一定是反应时数据，第二个就是分布。然后是根据条件，根据不同的条件进行画。因为我们有match这个条件，还有一个shape这个条件，所以我们总共有八个条件。也就是说，我们至少要提交八个图，反映一个base在八个条件下的反应时的分布。大家可以自由发挥，但是我们要看到八个图，这是基本的。大家提交的格式是要用R Markdown，记录下自己的画图思路。 当读取数据时，要与我们的PPT中的格式一致。因此，最好将你的文件夹组织结构与我们的课件完全相同。否则，我们将无法读取你的文件夹，这表明你的文件夹组织结构存在问题。如果你在GitHub上或之前保存过文件夹，那么这个文件夹应该包含我们的每张PPT，以HTML格式和RMD文件。对于第一次作业，直接将其放在这个文件夹中。我们需要能够在打开文件夹后读取和运行它，不要出现错误。你明白了吗？这意味着你的根目录相对路径必须与我们的保持一致。在第二课中，我们将讨论相对路径。如果你有任何疑问，请随时问。如果没有问题，那么今天的课就结束了。谢谢大家！ "],["lesson-7.html", "8 第七讲：如何进行基本的数据分析: t-test和anova 8.1 语法实现", " 8 第七讲：如何进行基本的数据分析: t-test和anova 8.1 语法实现 8.1.1 t检验 8.1.1.1 理论基础 我们来讲讲t检验。最简单的是单样本t检验，它是一个样本的一系列数据，将其平均值与一个固定值进行比较。我们有两个假设，H0和H1，假设显著水平为0.05。NHST的逻辑是，首先假设H0为真，然后从H0统计模型中提取当前数据，计算概率。如果概率很低，我们认为它不太可能发生，拒绝假设H0。p值与假设水平进行比较时，我们假设H0为真。我们有三个选择，等于、大于或小于。如果不等于，那么是双样本检验，如果小于或大于，则为单样本检验。绍的是这些检验的不同类型。首先是单样本检验，当p值小于等于mu时，H0大于等于mu。另一种单样本检验是当H0大于等于mu时。 接下来是双样本检验，假设两个样本来自同一个集合。我们可以用p值或t值检验来比较两个样本之间的差异。如果数据是高度相关的，我们可以使用相关数据的t值检验。独立样本t值检验也是双样本检验，它是两对无相关的数据。我们可以使用R语言的函数来进行这些检验。有各种R语言函数可以用于t检验，这些函数包括在R-base基础包中。我们将会介绍Bruce R包，它是为心理学优化设计的，使用它更方便。 我们载入所需的包。BruceR包载入后会有一大串提示信息，包括载入的包、主要函数和网址等。我们使用相对路径读取之前的数据，可以查看前5行。 ####解决问题 library(bruceR) ## Warning: package &#39;bruceR&#39; was built under R version 4.2.3 ## ## bruceR (v0.8.10) ## BRoadly Useful Convenient and Efficient R functions ## ## Packages also loaded: ## ✔ data.table ✔ emmeans ## ✔ dplyr ✔ lmerTest ## ✔ tidyr ✔ effectsize ## ✔ stringr ✔ performance ## ✔ ggplot2 ✔ interactions ## ## Main functions of `bruceR`: ## cc() Describe() TTEST() ## add() Freq() MANOVA() ## .mean() Corr() EMMEANS() ## set.wd() Alpha() PROCESS() ## import() EFA() model_summary() ## print_table() CFA() lavaan_summary() ## ## For full functionality, please install all dependencies: ## install.packages(&quot;bruceR&quot;, dep=TRUE) ## ## Online documentation: ## https://psychbruce.github.io/bruceR ## ## These packages are dependencies of `bruceR` but not installed: ## - cowplot, ggtext, lmtest, vars, phia, BayesFactor, GGally, GPArotation ## ## ***** Install all dependencies ***** ## install.packages(&quot;bruceR&quot;, dep=TRUE) ## ************************************ library(dplyr) library(tidyr) “在penguin数据中，参与者对于家的依恋水平是否小于/等于/大于均值水平(假设在总体水平上，人们对家庭的依恋水平(HOME)均值为3.5)？” “在penguin数据中，男女生在亲密关系经验(ECR)中的得分是否存在显著差异？” “在penguin数据中，参与者在不同时间的温度差异是否具有显著性？” WD &lt;- here::here() df.pg.raw &lt;- read.csv(&#39;./data/penguin/penguin_rawdata.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) 首先，我们需要进行数据预处理，计算家庭依恋的均值。这需要使用之前学过的数据预处理函数和方法，我们将使用管道进行操作。为了计算第二个问题，我们首先筛选出两个性别，即男性和女性。然后，我们对数据进行操作，计算两个变量的得分均值，用mutate生成两个新变量。我们使用命令na.rm来去除包含缺失值的数据，以避免出现错误。我们可以使用命令将性别转换为factor，以便在进行t-test时更容易分类。在使用R语言时，不同的函数可能会有微小的区别，因此我们需要注意这些细节。在选择数据时，应该只选取感兴趣的变量进行分析，如年龄、性别、亲密关系、问卷得分和温度测量值等。 df.pg.mean &lt;- df.pg.raw %&gt;% dplyr::filter(sex &gt; 0 &amp; sex &lt; 3) %&gt;% dplyr::mutate(ECR_mean = rowMeans(select(., starts_with(&quot;ECR&quot;)),na.rm = T), HOME_mean = rowMeans(select(., starts_with(&quot;HOME&quot;)),na.rm = T), sex=as.factor(sex) ) %&gt;% dplyr::select(sex, ECR_mean, HOME_mean, Temperature_t1,Temperature_t2) 进行t-test时，需要输入数据框、y变量和x变量，同时需要注意是否是配对样本和是否满足方向性。 bruceR::TTEST(df.pg.mean, &quot;HOME_mean&quot;, test.value = 3.5, test.sided = &quot;&gt;&quot;) ## ## One-Sample t-test ## ## Hypothesis: one-sided (μ &gt; 3.5) ## `BayesFactor` package needs to be installed. ## Descriptives: ## ─────────────────────────── ## Variable N Mean (S.D.) ## ─────────────────────────── ## HOME_mean 1490 3.99 (0.73) ## ─────────────────────────── ## ## Results of t-test: ## ────────────────────────────────────────────────────────────────────────────────────────────── ## t df p Difference [95% CI] Cohen’s d [95% CI] BF10 ## ────────────────────────────────────────────────────────────────────────────────────────────── ## HOME_mean: (HOME_mean - 3.5) 25.97 1489 &lt;.001 *** 0.49 [0.46, Inf] 0.67 [0.63, Inf] ## ────────────────────────────────────────────────────────────────────────────────────────────── bruceR::TTEST(df.pg.mean, &quot;HOME_mean&quot;, test.value = 3.5, test.sided = &quot;&lt;&quot;) ## ## One-Sample t-test ## ## Hypothesis: one-sided (μ &lt; 3.5) ## `BayesFactor` package needs to be installed. ## Descriptives: ## ─────────────────────────── ## Variable N Mean (S.D.) ## ─────────────────────────── ## HOME_mean 1490 3.99 (0.73) ## ─────────────────────────── ## ## Results of t-test: ## ────────────────────────────────────────────────────────────────────────────────────────────── ## t df p Difference [95% CI] Cohen’s d [95% CI] BF10 ## ────────────────────────────────────────────────────────────────────────────────────────────── ## HOME_mean: (HOME_mean - 3.5) 25.97 1489 1.000 0.49 [-Inf, 0.52] 0.67 [-Inf, 0.72] ## ────────────────────────────────────────────────────────────────────────────────────────────── result.ttest &lt;- capture.output({ bruceR::TTEST(data=df.pg.mean, y=&quot;HOME_mean&quot;, test.value = 3.5, test.sided = &quot;=&quot;, file = &quot;./output/chp7/single_t.doc&quot;) }) ## `BayesFactor` package needs to be installed. writeLines(result.ttest, &quot;./output/chp7/single_t.md&quot;) # .md最整齐 mean difference代表两组数据的均值差异，是t-test的一个效应量。test side表示双维还是单维，。test value是要检验的变量。factor variance表示是否对因素进行反转，digital参数指小数点后保留的位数，file参数用于保存结果到word文档。 使用capture output output函数可以整理结果并复制到results.ttest变量中。将结果复制到名为“results.ttest”的变量中，并将结果写入一个名为“single_t.md”的文件中。在根目录中打开R4SAC项目文件，我们可以看到已经运行过的输出结果，其中包括一个符合心理学格式的三线表格，其中包含了均值、t值、自由度、p值、协方差和效应量等信息。这个表格非常干净. result.ttest &lt;- capture.output({ bruceR::TTEST(data=df.pg.mean, y=&quot;ECR_mean&quot;, x=&quot;sex&quot;) }) ## `BayesFactor` package needs to be installed. writeLines(result.ttest, &quot;./output/chp7/inde_t.md&quot;) 然后是独立样本t检验，比较男性和女性之间的差异。使用了ECRmin作为样本得分，用性别作为分组变量。结果是two-side的，有描述性的结果，结果包括每组的n、均值和方差齐性检验。如果方差不齐性，可以使用校正方法。t检验的结，包括t值、df值、p值、mean difference result.ttest &lt;- capture.output({ bruceR::TTEST(data=df.pg.mean, y = c(&quot;Temperature_t1&quot;, &quot;Temperature_t2&quot;), paired = T) #是否为配对样本t检验？默认是FALSE }) ## `BayesFactor` package needs to be installed. writeLines(result.ttest, &quot;./output/chp7/pair_t.md&quot;) 配对样本t检验也很简单，只需要将y变成两个值，然后使用t1和t2进行检验。 请大家注意，我们之前讲过如何管理和放置你的R-Project，可以避免路径错误。如果你不熟悉路径，可以从github上下载整个文件夹，打开R4psy文件夹中地R-Project文件。我们的助教已经在群里分享了GitHub链接。打开链接后，点击绿色按钮下面的“code”，再点击“download zip”，就可以下载整个文件夹。下载后解压缩，把文件夹放在一个完整的文件夹里。文件夹里有一个叫做“R4psy”的文件，双击它就可以了。 有时候打开课程内容，运行代码会看到一个三角或加号，加号表示代码没有输完，可能是缺少反括号或不知道加号前面跟哪个对应。可以补上反括号或按退出键退出。Rstudio是一个好的IDE，可以帮助我们导航代码。在Rmarkdown里，代码块以chunk1、chunk2等命名，可以快速定位到代码框。有些chunk没有命名，只有编号。也可以通过outline来定位。 如果你想要快速运行代码，推荐使用快捷键control+回车，在Mac上是command+回车。也可以通过查找keyboard shortcut来找到更多快捷键。 在进行统计分析后，可以使用软件包reports来标准化报告结果，包括均值、标准差、t值、p值和Cohen’s d等。 8.1.2 方差分析 我们将使用Brusa和Tidyburst软件包，但需要注意，安装BaseFact包可能会出现错误，因为它依赖于Jax包。如果出现错误，可以在官方网站上搜索如何安装BaseFact包。注意安装依赖包，否则可能会出现问题。在使用Pinode时，可能会遇到一些警告和问题，需要自己解决。 df.match &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) %&gt;% # 拆分单元格内字符串 tidyr::separate(col=Shape, into=c(&quot;Valence&quot;,&quot;Identity&quot;), sep=&quot;(?&lt;=moral|immoral)(?=Self|Other)&quot;) %&gt;% dplyr::select(Sub, Valence, Identity, everything()) %&gt;% dplyr::filter(ACC == 1) %&gt;% # 只选择回答正确的数据 dplyr::filter(!is.na(RT)) %&gt;% # 剔除缺失值 # remove outliers below and above 3rd sd dplyr::filter(RT &gt; quantile(RT, 0.0015) &amp; RT &lt; quantile(RT, 0.9985)) %&gt;% dplyr::mutate(RT = as.numeric(RT)) %&gt;% dplyr::mutate(Valence = as.factor(Valence), Identity = as.factor(Identity)) WD &lt;- here::here() df.match.raw &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE)%&gt;% tidyr::separate(., col=Shape,into = c(&quot;Valence&quot;,&quot;Identity&quot;), sep = &quot;(?&lt;=moral|immoral)(?=Self|Other)&quot;)%&gt;% #拆分单元格内字符串 dplyr::select(Sub,Valence,Identity,everything())%&gt;% dplyr::filter(ACC == 1) %&gt;% #只选择回答正确的数据 dplyr::filter(RT &gt; quantile(RT, 0.0015) &amp; RT &lt; quantile(RT, 0.9985)) %&gt;% #remove outliers below and above 3rd sd dplyr::mutate(RT = as.numeric(RT)) %&gt;% dplyr::mutate(across(&quot;ACC&quot;, as.factor)) %&gt;% dplyr::mutate(Valence = as.factor(Valence)) %&gt;% dplyr::mutate(Identity = as.factor(Identity)) %&gt;% dplyr::filter(!is.na(RT)) #剔除缺失值 DT::datatable(head(df.match.raw, 10), fillContainer = TRUE, options = list(pageLength = 5)) 在数据清理时，可以先读取数据，然后进行管道操作，将Shape列分割成Valence和Identity两列，并选择Base、Valence、Identity和Everything列。然后进行数据清理，只选择反应正确的反应时间，并去掉缺失值和极端反应时间。我们使用了三个标准差的方法来去除极端值。我们将反应时间转换为数值型，将情感和身份转换为因子型。我们将所有操作应用于数据框df.match，这是经过预处理的数据。 df.match.mean &lt;- df.match.raw %&gt;% dplyr::group_by(Sub, Match) %&gt;% dplyr::summarise( n = n(), rt_sd = sd(as.numeric(RT), na.rm = T), rt_mean = mean(as.numeric(RT), na.rm = T), acc_mean=mean(as.numeric(as.character(ACC)),na.rm=T)#对于factor数据，先转成character，再变成numeric ) %&gt;% dplyr::ungroup() ## `summarise()` has grouped output by &#39;Sub&#39;. You can override using the `.groups` ## argument. 我们需要求出每个被试在匹配和不匹配任务上的均值，才能进行比较。为此，我们使用分组进行预处理，分组的目标是为了得到每个组的描述性统计，如RT的mean和SD，以及正确试次的个数。我们可以使用summarize快速得出这些统计信息。接着，我们可以使用T-test或方差分析来比较匹配和不匹配反应是否有差异。 #比较被试在匹配任务和不匹配任务上的反应时是否存在差异 #单因素被试内设计（长型数据） result.anova &lt;- capture.output({ df_within &lt;- df.match.mean%&gt;% bruceR::MANOVA(subID=&quot;Sub&quot;,#被试id dv=&quot;rt_mean&quot;,#因变量，因为是MANOVA，实际上因变量可以再后面设置很多个 within=&quot;Match&quot;) #设置条件，因素分析的条件 }) ## ## * Data are aggregated to mean (across items/trials) ## if there are &gt;=2 observations per subject and cell. ## You may use Linear Mixed Model to analyze the data, ## e.g., with subjects and items as level-2 clusters. writeLines(result.anova, &quot;./output/chp7/anova_1.md&quot;) # 若不符合球形假设要加上：sph.correction = &quot;GG&quot; 在方差分析中，我们可以使用BruceR包中的MANOVA()函数。参数分别是subID,dv和within，即可直接对其进行操作并输出结果。这几个参数符合心理学的命名规范。我们可以得到match和mismatch两种条件的描述性统计，包括f值、p值、eta2、pash2、95%置信区间和generalized eta2。 因为这是一个完全被试内设计的分析，所以不需要进行方差齐性检验。 df.match.mean &lt;- df.match %&gt;% dplyr::group_by(Sub,Sex, Valence,Identity) %&gt;% dplyr::summarise( n = n(), rt_sd = sd(as.numeric(RT), na.rm = T), rt_mean = mean(as.numeric(RT), na.rm = T) ) %&gt;% dplyr::ungroup() ## `summarise()` has grouped output by &#39;Sub&#39;, &#39;Sex&#39;, &#39;Valence&#39;. You can override ## using the `.groups` argument. 对于被试身份和效价这两个因素，我们可以直接从match中选出数值对，然后进行groupby和summarize操作。需要注意的是，我们现在是以subject、variance和identity为基础进行分析。 df.match.within &lt;- df.match.mean %&gt;% dplyr::select(-c(n, rt_sd)) %&gt;% dplyr::mutate(Valence = paste(&quot;A_&quot;, Valence, sep = &quot;&quot;),#将变量的名称进行修改转换，不生产新的变量 Identity = paste(&quot;B_&quot;, Identity, sep = &quot;&quot;)) %&gt;% # 将morality和identity组合名称起来生成一个新的变量&quot;Conds&quot; tidyr::unite(&quot;Conds&quot;, Valence:Identity, sep = &quot;&amp;&quot;,remove=TRUE) %&gt;% tidyr::pivot_wider(names_from = Conds, values_from = rt_mean) head(df.match.within) ## # A tibble: 6 × 6 ## Sub Sex `A_immoral&amp;B_Other` `A_immoral&amp;B_Self` `A_moral&amp;B_Other` A_mora…¹ ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7302 female 0.706 0.724 0.662 0.721 ## 2 7303 male 0.741 0.754 0.720 0.700 ## 3 7304 female 0.756 0.730 0.757 0.628 ## 4 7305 male 0.685 0.659 0.641 0.638 ## 5 7306 male 0.778 0.763 0.660 0.771 ## 6 7307 female 0.751 0.677 0.716 0.728 ## # … with abbreviated variable name ¹​`A_moral&amp;B_Self` #str(df.match.within) #被试在不同身份(self vs other)与不同效价(moral vs moral)的条件组合下反应时是否存在差异） result.anova &lt;- capture.output({ res_rmANOVA_1 &lt;- bruceR::MANOVA(data=df.match.within, dvs=&quot;A_immoral&amp;B_Other:A_moral&amp;B_Self&quot;, dvs.pattern=&quot;A_(.+)B_(.+)&quot;,#正则表达式 within=c(&quot;A_&quot;,&quot;B_&quot;))#表示2个主条件 }) ## ## Note: ## dvs=&quot;A_immoral&amp;B_Other:A_moral&amp;B_Self&quot; is matched to variables: ## A_immoral&amp;B_Other, A_immoral&amp;B_Self, A_moral&amp;B_Other, A_moral&amp;B_Self writeLines(result.anova, &quot;./output/chp7/anova_2.md&quot;) 如果我们有的是宽数据，我们可以先将宽数据转成长数据，然后选择变量并按照Bruce R的命名方式在前面加上a和b表示自变量a和自变量b。接着，我们可以使用unite函数将Valence和Identity合并成一个新的变量，再使用pivot wide将其转换为更宽的形式。这样，我们就可以看到每个条件下的性别、被试id和条件。 最后，我们将结果写成md格式，得到一个简洁的表格，其中包含f值、p值、partial square、generalized square和partial square的95%置信区间。长数据也可以使用同样的方法处理。 #被试在不同身份(self vs other)与不同效价(good vs bad)的条件组合下反应时是否存在差异） #head(df.match.mean) result.anova &lt;- capture.output({ res_rmANOVA_2 &lt;- bruceR::MANOVA(data=df.match.mean, dv=&quot;rt_mean&quot;, within=c(&quot;Valence&quot;,&quot;Identity&quot;), subID=&quot;Sub&quot;) }) ## ## * Data are aggregated to mean (across items/trials) ## if there are &gt;=2 observations per subject and cell. ## You may use Linear Mixed Model to analyze the data, ## e.g., with subjects and items as level-2 clusters. writeLines(result.anova, &quot;./output/chp7/anova_3.md&quot;) result.check &lt;- capture.output({ sim_eff_1 &lt;- res_rmANOVA_1 %&gt;% bruceR::EMMEANS(&quot;A_&quot;, by=&quot;B_&quot;)#简单效应分析 }) writeLines(result.check, &quot;./output/chp7/check.md&quot;) #sim_eff_1 &lt;- res_rmANOVA_1 %&gt;% #EMMEANS(&quot;B_&quot;, by=&quot;A_&quot;) 和上一个同理，只是转换了不同的形式 除了基础分析方法，我们还可以使用EMMEANS进行简单效应分析和多重比较矫正等操作。通过输入之前的方差分析结果，我们可以使用EMMEANS a by b来进行简单效应分析，以比较在不同条件下的主效应是否存在差异。 我们要分析a在b的不同条件下的效应，包括other和self条件。在other条件下，a的效应是b条件的一个主效应，我们称之为简单效应。我们可以看到，在b的两个条件下，a的简单效应实际上应该有一个交互作用。在other条件下，它不显著，在self条件下，它很显著。 此外，我们还可以看到a的quantity，即在self条件下，a的主效应moral和immoral之间差异非常大，quantity大于0.8，是一个大的效应。 "],["lesson-8.html", "9 第八讲：如何进行基本的数据分析: 相关与回归 9.1 什么是相关 9.2 相关-代码实现 9.3 什么是回归", " 9 第八讲：如何进行基本的数据分析: 相关与回归 9.1 什么是相关 相关分析是一种统计技术，用于测量两个变量之间线性关系的强度和方向。它涉及计算相关系数，这是一个范围从-1到1的值，其中-1表示完全的负相关，0表示无相关，1表示完全正相关。 Pearson(皮尔逊)相关 Pearson相关系数是最常用的方法之一，用于衡量两个变量之间的线性相关程度，取值范围为-1到1之间，其值越接近于1或-1表示两个变量之间的线性相关程度越强，而越接近于0则表示两个变量之间线性相关程度越弱或不存在线性相关性。 Spearman(斯皮尔曼)相关 Spearman等级相关系数用于衡量两个变量之间的关联程度，但不要求变量呈现线性关系，而是通过对变量的等级进行比较来计算它们之间的相关性。 Kendall(肯德尔)相关 Kendall秩相关系数也用于衡量两个变量之间的关联程度，其计算方式与Spearman等级相关系数类似，但它是基于每个变量的秩的比较来计算它们之间的相关性。 假想问题 在penguin数据中，参与者压力和自律水平的相关水平？ 9.2 相关-代码实现 # 检查是否已安装 pacman if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)) { install.packages(&quot;pacman&quot;) } # 如果未安装，则安装包 # 使用p_load来载入需要的包 pacman::p_load(&quot;tidyverse&quot;, &quot;bruceR&quot;, &quot;performance&quot;) # 或者直接使用 easystats这个系列 pacman::p_load(&quot;tidyverse&quot;, &quot;bruceR&quot;, &quot;easystats&quot;) 检查工作路径 - 导入原始数据 # 检查工作路径 getwd() ## [1] &quot;C:/GitHub/R4PsyBook/bookdown_files/Books/Book&quot; #读取数据 df.pg.raw &lt;- read.csv(&#39;./data/penguin/penguin_rawdata.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) 我们需要将数据导入R中，并进行数据清洗和转换。然后，我们可以使用Tidyverse包中的函数来选择和转换数据。在进行反向计分后，我们可以使用mutate函数来计算每个问卷的得分。 接下来，我们选择性别、压力和自我控制这三个变量，并使用Bruce R中的相关分析方法来计算它们之间的相关性。需要注意的是，当我们有多个变量需要进行两两相关性分析时，需要进行P值的多重性校正。我们最后得到的是一个宽数据，我们可以在此基础上进行进一步的分析。 df.pg.corr &lt;- df.pg.raw %&gt;% dplyr::filter(sex &gt; 0 &amp; sex &lt; 3) %&gt;% # 筛选出男性和女性的数据 dplyr::select(sex, starts_with(&quot;scontrol&quot;), starts_with(&quot;stress&quot;)) %&gt;% # 筛选出需要的变量 dplyr::mutate(across(c(scontrol2, scontrol3,scontrol4, scontrol5,scontrol7, scontrol9, scontrol10,scontrol12,scontrol13, stress4, stress5, stress6,stress7, stress9, stress10,stress13), ~ case_when(. == &#39;1&#39; ~ &#39;5&#39;, . == &#39;2&#39; ~ &#39;4&#39;, . == &#39;3&#39; ~ &#39;3&#39;, . == &#39;4&#39; ~ &#39;2&#39;, . == &#39;5&#39; ~ &#39;1&#39;, TRUE ~ as.character(.))) ) %&gt;% # 反向计分修正 dplyr::mutate(across(starts_with(&quot;scontrol&quot;) | starts_with(&quot;stress&quot;), ~ as.numeric(.)) ) %&gt;% # 将数据类型转化为numeric dplyr::mutate(stress_mean = rowMeans(select(.,starts_with(&quot;stress&quot;)), na.rm = T), scontrol_mean = rowMeans(select(., starts_with(&quot;scontrol&quot;)), na.rm = T) ) %&gt;% # 根据子项目求综合平均 dplyr::select(sex, stress_mean, scontrol_mean) 查看一下前五行 bruceR::Corr() results.Corr &lt;- capture.output({ bruceR::Corr(data = df.pg.corr[,c(2,3)], file = &quot;./output/chp8/Corr.doc&quot;) }) writeLines(results.Corr, &quot;./output/chp8/Corr.md&quot;) # .md最整齐 Bruce R默认的采用的是pearson相关，那么你也可以选择使用spearman或者kendall 然后另外如果我们有多个,比方说我们有三个变量，两两之间计算相关的话 那么我们往往是要进行这个P值的多重性的校正的，假如说我们只有两个变量的话 那肯定是不需要了 因为不存在这个多重比较较真的问题，假如说我们有三个以上变量，这个肯定是要校正的。 在保存文件时，要注意文件名和文件类型的后缀。 我们也可以用散点图来展示变量之间的相关性。 #绘制相关散点图 pairs(df.pg.corr[,c(2,3)]) 我们RStudio有4个panel，最右下角的这个面板是用来画图的，有的时候大家可能会觉得发现画的图没有输出，可能就是这个地方 绘图的区域留的不够大，如果这个地方留的不够大的话，有可能就画不出来。 运行完了之后它会自动的输出一个图 我们可以看到就是stress_mean和control_mean，它自动会以一个矩阵的形式来输出一个相关矩阵 然后我们可以看到这里有一个从-1到1的一个 legend图例，颜色越浅的话表示相关越低，它应该会把显著的相关会标出来，如果没有记错的话，这个地方可以看到它这里显示的数字 就是它的相关系数0.05，这是一个相当小的一个相关系数。 我们其实也可以把它输出为一个word文档，需要注意的是我们可能需要精确的指出我们以哪些columns作为输入 假如说大家不选择的话，它会把性别和我们的别的变量也做相关,与其他的两个连续变量也会做相关。但这样的情况是不能用pearson的。 假如说们有一个变量是男性和女性，另外一个变量是连续的变量，我们不能直接用pearson做相关，我们应该用点二类相关，或者直接使用t检验。 那么在ezstates里面有一个类似的包叫做correlation，那么correlation它会有一个好处就是它会输出更多的信息，尤其当我们做探索性的分析的时候。我们前面讲探索性的分析，假如说我们对这个变量并不是特别清楚，哪些变量之间有相关，我们想去探索一下。 它就会以这样一个可说话的方式告诉我，哪两个变量之间相关变强，哪两个变量相对弱，那么我一下可能就能看出变量之间的一个模式。 9.3 什么是回归 回归模型是通过对观测数据进行拟合来描述变量之间的关系。回归允许我们估计因变量如何随着自变量的变化而变化。 我们说线性模型, 基本上是我们整个经典统计, 包括我们所有的T-test,LOVA,还有相关,还有我们说的回归分析，包括单变量,多变量,回归分析,它所有的都是有一个共同的技术, 我们说它叫做GLM,Generalized Linear Model, 或者更加复杂点叫做Generalized Linear Mixed Model,它本质上就是Y=A+BX。我们通常会将Y写成为预测项，有时候我们会在预测项上加上一个误差，这是可以扩展的，我们也可以假设他是一个非线性的关系，当它是线性的时候，我们实际上是在预测一个正态分布的均值，如果我们不是预测均值，我们可以通过一个转换，使用一个链接函数，转化后的参数仍然能用这种方法来组合预测，这个自变量或者因变量可以是非连续的变量。 ## 回归-代码实现 \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon\\] 假想的问题 “在penguin数据中，我们希望找到参与者压力和自律水平关于性别的回归？” # 数据预处理 df.pg.lm &lt;- df.pg.corr %&gt;% # 将sex变量转化为factor类型 mutate(sex = as.factor(sex)) %&gt;% # 自变量为scontrol和sex group_by(scontrol_mean,sex) %&gt;% # 根据分组获得stress的平均值，。groups属性保留了之前的group_by summarise(stress_Mean = mean(stress_mean),.groups = &#39;keep&#39;) 如果我们希望找到压，自律以及性别之间的一个关系, 我们先做了correlation，然后把性别转换一下,然后再把数据进行group by, 这是我们的数据预处理的过程。 数据预处理之后的话,我们其实首先可以做一个探索分析，它们是不是有一个关系? 那么我们可以先用ggPlot画一个图，把男性和女性的数据就分别以不同的颜色画出来。 可以看到这里面我们用geom_smooth(), 当我们绘制了散点，需要继续绘制趋势线的时候,他自动使用了一个formula,y ~ x，这就表示用x来预测y。这时候的y就是strength， x就是self-control。两条线看起来是交叉的，这说明 在这个女性当中是有一个相关, 在男性当中好像没有这个相关，那么我们后面可以对它进行一个检验。 # 使用ggplot()画图 ggplot(df.pg.lm, aes(x = scontrol_mean, y = stress_Mean, color = sex)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + scale_color_discrete(name = &quot;Gender&quot;, labels = c(&quot;Female&quot;, &quot;Male&quot;)) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 那么我们直接使用lm linear model。它实际上是base里面的一个函数。这实际上是R里面写回归的一个常用的公式的写法, 就是说我们把一边选自变量,对吧,一边选因变量,然后另外一个选自备量,然后它就自动给我们输出结果。 那么在R里面的话,你必须要自己把这个公式写清楚。 我们使用主观压力stress作为一个因变量， 公式右边也就是我们的自变量，分别是sex性别和scontrol_mean，还有第三个自变量就是性别和控制的一个交互作用对吧,因为我们有两个自备量,所以两个自备量之间它可能会有交互作用。 这种写法大家如果说你后面长期要用它,然后做线性回归,或者是基于线性回归的一些方法的话,这个可能你还是最好要了解一下。 我们的公式会作为第一个参数被输入到函数里面。你也可以更完善的补全它的argument，它应该就是formula等于这个后面的一串。 然后后面就是data等于df什么什么,这个地方所以我们基本上都把这两个argument给省略掉了。 那然后的话我们其实你运行了这个代码之后的话,它就会把这个结果,会把这个线性回归的结果存到一个叫mod的变量里去,就在我们电脑上面了对吧,内存里面了。 那这个时候的话我们可以就是用mod_summary,然后呢来去把它把它结果提取出来。 # 建立回归模型 mod &lt;- lm(stress_Mean ~ scontrol_mean + sex + scontrol_mean:sex, df.pg.lm) # 使用bruceR::model_summary()输出结果 result.lm &lt;- capture.output({ model_summary(mod, std = T, file = &quot;./output/chp8/Lm.doc&quot;) }) writeLines(result.lm, &quot;./output/chp8/Lm.md&quot;) 这是我们的显著性检验，我们可以看到selfcontrol的预测作用和sex的预测作用以及它们之间的交互作用。他们的显著性可以在这里看到，然后呢我们看这个决定系数对吧,这个整个模型的一个决定系数,解释了多少变异,然后adjusted就是调整之后的,还有number of observations 53。 我们推荐一个很好的包：performance,这个performance是专门为了解决我们常用的这个统计里面的这个模型,包括像我们Anova T-test等模型。它会检查我们这个模型的各个方面,比如posterior predictive check，inlinearity，方差齐性，共线性等等。 还有正态性的一个检验对吧,这里面会有各种各样的一些检验,那么大家如果需要去做完这个模型之后,需要去对自己的模型是不是符合做这个模型的一个假定一个assumption。 它还可以进行模型比较,比如我们有模型1是有交互作用的,模型2是没有交互作用的。然后甚至比方说模型3和5只有性别的一个自变量作用,我们有三个模型,我们能够看到哪个模型能够更合理的解释数据的变异,可以用model comparison这个函数来对三个模型进行比较,这个在easy state里面有的。 那么关于这部分的话,我们就是说有两个需要强调的,第一个就是说我们的公式如何写,对于第一次在R里面接触回归模型的同学来说,我们可能要注意,它是这么一个固定的套路。 然后第二个就是我们如果说是用简单的回归的话,那基本上用LM这个就可以了,然后大家在心理统计学,一个变量和另外一个变量的预设作用或者多个变量和一个变量的预设作用LM都是可以做到的。 然后比方说甚至你这个变量类型,像我们这里放了一个二分的变量对吧,它也是可以的。 "],["references.html", "References", " References "]]
